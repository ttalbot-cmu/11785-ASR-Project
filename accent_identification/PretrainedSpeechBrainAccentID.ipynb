{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwZGjYRx1MaK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# installation\n",
        "!git clone https://github.com/speechbrain/speechbrain/\n",
        "%cd /content/speechbrain/\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TirnYoM1h6d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# For pip installation\n",
        "!pip install speechbrain\n",
        "!mkdir \"/content/results\"\n",
        "!mkdir \"/content/common_voice\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ29Q4Fk2wVo"
      },
      "source": [
        "Load pretrained speech-brain model. \n",
        "Speech brain has 68 different models to choose from listed on hugging face ~ https://huggingface.co/speechbrain "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2kihRfmE0zTX"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "# Model is downloaded from the speechbrain HuggingFace repo (There could be others that work better).\n",
        "\n",
        "classifier = EncoderClassifier.from_hparams(\n",
        "  source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "  savedir=\"/content/results\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpMtLkEe4V7r"
      },
      "source": [
        "(Optional) Load the english common voice dataset from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ldJQOwg5Y7L",
        "outputId": "783fc0b0-6953-4597-ad7e-b5a04b3fb967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of mp3 file names is 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# TODO: Add in the mfccs and transcripts for the finetuning. \n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.waveforms = []\n",
        "        self.accents = []\n",
        "\n",
        "        mp3_file_names = os.listdir(root)\n",
        "        print(\"The length of mp3 file names is {}\".format(len(mp3_file_names)))\n",
        "        mp3_file_names.sort()\n",
        "        for i in range(len(mp3_file_names)):\n",
        "            waveform, sample_rate = torchaudio.load(root + mp3_file_names[i])\n",
        "            print(\"The shape of waveforms is {}\".format(waveform.shape))\n",
        "            self.waveforms.append(waveform)\n",
        "            self.accents.append(\"TODO?\")\n",
        "        \n",
        "        assert(len(self.waveforms) == len(self.accents))\n",
        "        self.length = len(self.waveforms)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        waveform = self.waveforms[ind]\n",
        "        return waveform, self.accents[ind]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      batch_waveforms = []\n",
        "      batch_lengths = []\n",
        "      for i in range(len(batch)):\n",
        "          batch_waveforms.append(batch[i]) \n",
        "          batch_lengths.append(len(batch[i]))\n",
        "\n",
        "      return torch.tensor(batch_waveforms), torch.tensor(batch_lengths)\n",
        "\n",
        "common_voice_dataset = AudioDataset(\"/content/common_voice/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_3gAtIf6AYs"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YTIg_QLn5_6G"
      },
      "outputs": [],
      "source": [
        "common_voice_data_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = common_voice_dataset, \n",
        "    num_workers = 2, \n",
        "    batch_size  = 1, \n",
        "    pin_memory  = True, \n",
        "    shuffle     = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kZa-Ks44fih"
      },
      "source": [
        "Compute embeddings for the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "KGNqOcWr4sdW",
        "outputId": "4daab880-4cc0-4ec1-abca-ac3ab9aa340c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of embeddings is 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f85709070a1e>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The length of embeddings is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The shape of embedding 0 is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "embeddings = []\n",
        "for i, (waveform_batch, accent_batch) in enumerate(common_voice_data_loader):\n",
        "  \n",
        "  # The arguments to encode batch should be wav, wav_lens\n",
        "  # wav is shape [batch_size, time, num_channels] or just [batch_size, time]\n",
        "  # wav_lens is a torch.Tensor of shhape [batch] the longest recording in the \n",
        "  # batch has length 1.0. All other waveforms have a relative length to the \n",
        "  # max length. i.e. waveform.shape[1]/len(max_length)\n",
        "  #print(type(waveform_batch))\n",
        "  #print(waveform_batch.shape)\n",
        "  #waveform_batch.permute(0,2,1)\n",
        "  waveform_lens = torch.tensor([1.0])\n",
        "  embedding = classifier.encode_batch(waveform_batch, waveform_lens)\n",
        "  waveform = waveform_batch.squeeze()\n",
        "  embedding = classifier.encode_batch(waveform)\n",
        "  embeddings.append(embedding)\n",
        "\n",
        "print(\"The length of embeddings is {}\".format(len(embeddings)))\n",
        "print(\"The shape of embedding 0 is {}\".format(embeddings[0].shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxZdqPnkIPte"
      },
      "source": [
        "TODO: Create a new tensor which contains the original recording concatenated with the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHrU1PrxDlBw"
      },
      "source": [
        "Finetune the Whisper ASR model with the embedding + original common voice waveform. I am following the tutorial from ~ https://huggingface.co/blog/fine-tune-whisper. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQCAUT4wKaco"
      },
      "source": [
        "Load Pretrained Whisper Model for ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "vDvIMF-TG40f",
        "outputId": "45a9179f-9e6f-4aa4-9975-fb736e333c2b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-334ca642c25d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWhisperFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhisperFeatureExtractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openai/whisper-small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hgJpT8kOrVm"
      },
      "source": [
        "Apply the data preparation function over the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axCxj-dSO8wM"
      },
      "source": [
        "Define a data collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSGSqI4qD4RT"
      },
      "source": [
        "Perform inference using the GMU Speech Accent Archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiZMLtgUD3PI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}