{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k44fylYFktg"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FeMYucCiSr0b"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\" : 1e-6  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yv1FeoIRbG0"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bxd0gRCbRU5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Mon May  1 18:25:04 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W /  70W |   1247MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     19963      C   ...vs/pytorch_p39/bin/python     1243MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EL5wEwuZRWsi"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb --quiet\n",
    "# !pip install torchsummaryX -q\n",
    "# !pip install mutagen\n",
    "# !pip install jiwer\n",
    "# !pip install git+https://github.com/openai/whisper.git \n",
    "# # on Ubuntu or Debian\n",
    "# !sudo apt update && sudo apt install ffmpeg\n",
    "# !pip install -qqq evaluate==0.2.2\n",
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ['CUDA_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mE6sHivTH36m",
    "outputId": "175e610a-da43-46ee-b356-1ad728257611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from mutagen.mp3 import MP3\n",
    "import jiwer\n",
    "\n",
    "import wandb\n",
    "import evaluate\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDwpf5XT_1GP"
   },
   "source": [
    "## Load the pretrained Whisper model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6dIP0MrBFks",
    "outputId": "98526568-a2cd-417a-fe52-1343352d0827"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PXfGq1r4ja4m"
   },
   "outputs": [],
   "source": [
    "# hard-coded audio hyperparameters\n",
    "# SAMPLE_RATE = 16000\n",
    "# N_FFT = 400\n",
    "# N_MELS = 80\n",
    "# HOP_LENGTH = 160\n",
    "# CHUNK_LENGTH = 30\n",
    "# N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "# N_FRAMES = whisper.utils.exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "# N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "# FRAMES_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "# TOKENS_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr = librosa.load('/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_663624.mp3', sr=48000)\n",
    "# y_16k = librosa.resample(y, orig_sr=sr, target_sr=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtdVyWyGBqH"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8EF7Hc4ikiR2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # Increase if your device can handle it\n",
    "num_updates = 200\n",
    "val_updates = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"log.txt\",\"w\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of tensor a (2703) must match the size of tensor b (448) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mfcc = whisper.audio.log_mel_spectrogram(\"/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_17888668.mp3\",padding=N_SAMPLES)\n",
    "# # mfcc = whisper.audio.pad_or_trim(mfcc, N_FRAMES).to(DEVICE).to(dtype)\n",
    "# # print(mfcc.shape)\n",
    "# filepath = '/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_20266264.mp3'\n",
    "# orig_audio, sr = librosa.load(filepath, sr=48000)\n",
    "# whisper_audio = librosa.resample(orig_audio, orig_sr=sr, target_sr=16000)\n",
    "# # mfcc = whisper.audio.log_mel_spectrogram(whisper_audio)\n",
    "# # mfcc = whisper.audio.pad_or_trim(mfcc)\n",
    "# mfcc = whisper.audio.log_mel_spectrogram(whisper_audio,padding = 480000)\n",
    "# mfcc = whisper.audio.pad_or_trim(mfcc, 3000).to(DEVICE).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc = mfcc.reshape(1,80,3000)\n",
    "# h, lh = asrmodel(mfcc, torch.zeros(1,10,dtype=torch.long).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oAEEkVi2FiY0"
   },
   "outputs": [],
   "source": [
    "class AudioDataloader(torch.utils.data.DataLoader):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    \n",
    "    def __init__(self, partition, root, tokenizer, batch_size, shuffle): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        self.df = pd.read_csv('common_voice_'+partition+'.csv')\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_len = len(self.df)\n",
    "        self.root = root\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return round((self.df_len//self.batch_size)*0.95)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac = 1)\n",
    "        num_batches = self.__len__()\n",
    "        batch_idx = 0\n",
    "        batch_mfccs = []\n",
    "        batch_transcripts = []\n",
    "        batch_probs = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "        for index, entry in self.df.iterrows():\n",
    "            filepath = self.root + entry['path']\n",
    "            audio = MP3(filepath)\n",
    "            if audio.info.length < 28:\n",
    "                # f.write(filepath+\"\\n\")\n",
    "                transcript = torch.tensor(tokenizer.encode(entry['sentence']))\n",
    "                prob = entry['probs']\n",
    "                orig_audio, sr = librosa.load(filepath, sr=48000)\n",
    "                whisper_audio = librosa.resample(orig_audio, orig_sr=sr, target_sr=16000)\n",
    "                # mfcc = whisper.audio.log_mel_spectrogram(whisper_audio)\n",
    "                # mfcc = whisper.audio.pad_or_trim(mfcc)\n",
    "                mfcc = whisper.audio.log_mel_spectrogram(whisper_audio,padding = 480000)\n",
    "                mfcc = whisper.audio.pad_or_trim(mfcc, 3000).to(DEVICE).to(dtype)\n",
    "                batch_mfccs.append(mfcc)\n",
    "                lengths_mfcc.append(len(mfcc))\n",
    "                batch_transcripts.append(transcript)\n",
    "                lengths_transcript.append(len(transcript))\n",
    "                batch_probs.append(prob)\n",
    "            if len(batch_probs)== self.batch_size:\n",
    "                # f.write(\"new batch\\n\")\n",
    "                batch_idx += 1\n",
    "                batch_mfcc_pad = pad_sequence(batch_mfccs, batch_first= True) # TODO\n",
    "                # max_label_len = max(lengths_transcript)\n",
    "                # batch_transcript_pad = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(batch_transcripts, lengths_transcript)]\n",
    "                batch_transcript_pad = pad_sequence(batch_transcripts, batch_first= True) # TODO\n",
    "                batch_transcript_pad_ent = pad_sequence(batch_transcripts, batch_first= True, padding_value = 0) # TODO\n",
    "                yield batch_mfcc_pad, batch_transcript_pad, torch.tensor(batch_probs), torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript),batch_transcript_pad_ent\n",
    "                batch_mfccs = []\n",
    "                batch_transcripts = []\n",
    "                batch_probs = []\n",
    "                lengths_mfcc = []\n",
    "                lengths_transcript = []\n",
    "        self.length = len(self.mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iLYG4uhNkdFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches 59530\n",
      "val batches 14882\n"
     ]
    }
   ],
   "source": [
    "# Create objects for the dataset class\n",
    "root = '/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/'\n",
    "# train_data = AudioDataset('train',root, tokenizer) #TODO\n",
    "# val_data = AudioDataset('validation',root, tokenizer) # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "\n",
    "# # Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = train_data, \n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "# #     pin_memory  = True,\n",
    "#     shuffle     = True,\n",
    "#     collate_fn  = train_data.collate_fn\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = val_data,\n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "#     # pin_memory  = True,\n",
    "#     shuffle     = False,\n",
    "#     collate_fn  = val_data.collate_fn\n",
    "# )\n",
    "\n",
    "# print(\"Batch size: \", BATCH_SIZE)\n",
    "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "train_loader = AudioDataloader('train', root, tokenizer, BATCH_SIZE, shuffle = True)\n",
    "val_loader = AudioDataloader('validation', root, tokenizer, BATCH_SIZE, shuffle = True)\n",
    "print(\"train batches\", len(train_loader))\n",
    "print(\"val batches\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "vTeyWdnTkruv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80, 3000]) torch.Size([10, 23]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 80, 3000]) torch.Size([10, 24]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "x, y, p, lx, ly, yc = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "# print(x,y,p,lx,ly)\n",
    "x, y, p, lx, ly, yc = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "# print(x,y,p,lx,ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5zCZ11eSFws"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BjQVLuTtIMX2"
   },
   "outputs": [],
   "source": [
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.options = whisper.decoding.DecodingOptions(fp16 = True,temperature=0.0,without_timestamps=True)\n",
    "        self.asr = model\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # print(\"x type\",type(x))\n",
    "        # print(\"x shape\",x.shape)\n",
    "        # print(\"opt type\",type(self.options))\n",
    "        # print(\"opt shape\",self.options.shape)\n",
    "        # print(\"length\",lengths_x)\n",
    "        audio_features = self.asr.encoder(x)\n",
    "        results = self.asr.decoder(y, audio_features)\n",
    "        # print(results.shape,results)\n",
    "        results = results.log_softmax(2)\n",
    "        # print(results.shape,results)\n",
    "        # out = [torch.LongTensor(result.tokens).to(DEVICE) for result in results]\n",
    "        # out = torch.nn.utils.rnn.pad_sequence(out, batch_first=True, padding_value=0.0)\n",
    "        lx = self.asr.decode(x,self.options)\n",
    "        # print(\"orig len\", len(out))\n",
    "        lx = list(map(lambda x: len(x.tokens), lx))\n",
    "        # seq_unpacked, lens_unpacked = pad_packed_sequence(out, batch_first = True)\n",
    "        # print(\"new len\",len(out))\n",
    "        lx = torch.LongTensor(lx).to(DEVICE)\n",
    "        # return out, lengths_x\n",
    "        # return seq_unpacked, lens_unpacked\n",
    "        # print(type(out))\n",
    "        # print(out.shape)\n",
    "        # out = out.double()\n",
    "        # return out, lengths_x\n",
    "        return results,lx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvLOwGjISJT1"
   },
   "source": [
    "## Criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xARobkRVLMNr"
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True) # Define CTC loss as the criterion. How would the losses be reduced?\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'none', ignore_index = 0)\n",
    "optimizer =  torch.optim.SGD(model.parameters(),lr = config[\"lr\"]) # What goes in here?\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.5, total_iters=len(train_loader), last_epoch=- 1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2) #TODO\n",
    "# metric = evaluate.load(\"wil\")\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbkJqcWPSQnS"
   },
   "source": [
    "## Train and validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, val_batches):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=val_batches, dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    val_wil = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        if i == val_batches:\n",
    "            break\n",
    "        x, y, p, lx, ly, yc = data\n",
    "        x, y, p, yc = x.to(DEVICE), y.to(DEVICE), p.to(DEVICE), yc.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h,lh = model(x, y)\n",
    "            h = torch.permute(h, (0, 2, 1))\n",
    "            loss = criterion(h, yc)\n",
    "            loss = torch.sum(loss,dim=1)\n",
    "            loss = torch.mean(loss)\n",
    "            # loss = torch.mean(loss)\n",
    "        total_loss += float(loss)\n",
    "        # val_wil += wil(h,y)\n",
    "        \n",
    "        ht = model.asr.decode(x,model.options)\n",
    "        ht = list(map(lambda x: x.tokens, ht))\n",
    "        text_x,text_y = [],[]\n",
    "        for j in range(len(ht)):\n",
    "            val_wil += jiwer.wil(tokenizer.decode(ht[j]), tokenizer.decode(y[j]))/len(ht)            \n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), wil=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, yc, ht, text_x, text_y, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/ val_batches\n",
    "    avg_wil = val_wil/ val_batches\n",
    "    return total_loss, avg_wil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNcyukhDSUe7"
   },
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-ETu7LPQPy8W"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         metric[0]                  : metric[1], \n",
    "         'epoch'                    : epoch}, \n",
    "         path\n",
    "    )\n",
    "\n",
    "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCgcYFPWSZOF",
    "tags": []
   },
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "FFUj2WUVQP3f"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "best_wil = 1 # if you're restarting from some checkpoint, use what you saw there.\n",
    "epoch_model_path = \"/home/ubuntu/11785-ASR-Project/finetune/reg_epoch_model_05_01.checkpoint\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
    "best_model_path = \"/home/ubuntu/11785-ASR-Project/finetune/reg_best_model_05_01.checkpoint\"#TODO set best model path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(best_model_path)\n",
    "# print(checkpoint['val_wil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NaMfgXC8RNVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshilinm\u001b[0m (\u001b[33mdeeper_learners\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"a5b7420abbe354e6d0b2f5554b97ee11f327fc92\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(best_model_path, asrmodel, metric= 'val_wil', optimizer= None, scheduler= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "FjMI5MgYRN4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/11785-ASR-Project/finetune/wandb/run-20230501_182655-w4e27n1g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/w4e27n1g' target=\"_blank\">reg-test-run-clip</a></strong> to <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/w4e27n1g' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune/runs/w4e27n1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"reg-test-run-clip\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # id = 'oshmtgys', #Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"whisper-finetune\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq87gxHcScqH"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrmodel = ASRModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_wil = validate_model(asrmodel,val_loader, 10)\n",
    "# print(\"start val loss\", val_loss, \"start val wil\", val_wil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3JIKc4mQdP7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 192.6933\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4553\t Val Loss 187.7917\n",
      "saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 191.1729\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4445\t Val Loss 185.2332\n",
      "saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 189.6922\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4767\t Val Loss 187.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 188.3834\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4583\t Val Loss 189.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 182.3511\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4623\t Val Loss 188.2877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 181.8067\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4645\t Val Loss 177.6527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 179.8024\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4369\t Val Loss 179.6180\n",
      "saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 181.0475\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4439\t Val Loss 174.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 179.1866\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4648\t Val Loss 178.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 174.6558\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4224\t Val Loss 176.6700\n",
      "saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 171.9307\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4310\t Val Loss 173.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 171.4987\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4454\t Val Loss 167.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 170.1952\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4470\t Val Loss 165.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 167.4052\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4576\t Val Loss 167.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 166.4105\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4473\t Val Loss 163.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 163.6367\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4473\t Val Loss 162.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 164.1850\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4186\t Val Loss 167.6383\n",
      "saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 162.3650\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4511\t Val Loss 164.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 160.1531\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4447\t Val Loss 154.8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 159.6156\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4623\t Val Loss 153.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 143.1533\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4564\t Val Loss 142.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 140.6614\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4557\t Val Loss 140.7226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 140.1186\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4543\t Val Loss 141.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 137.3978\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4409\t Val Loss 139.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 136.0693\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4364\t Val Loss 136.5470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 133.9909\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.4497\t Val Loss 135.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 134.5559\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4402\t Val Loss 130.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   8%|▊        | 17/200 [00:16<02:37,  1.16it/s, loss=0.3721, lr=0.000001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 131.9264\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4631\t Val Loss 133.7734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 130.6070\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4367\t Val Loss 129.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 131.5987\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4529\t Val Loss 127.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 127.3998\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4440\t Val Loss 122.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 126.1339\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4352\t Val Loss 123.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 126.6517\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4481\t Val Loss 126.7644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 124.4395\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4465\t Val Loss 126.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 125.1267\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4510\t Val Loss 126.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 121.6136\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4393\t Val Loss 126.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 120.2664\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4480\t Val Loss 118.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 122.0770\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4321\t Val Loss 119.4825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 118.6865\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4380\t Val Loss 117.6606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 116.6916\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4592\t Val Loss 115.3483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 114.5651\t Learning Rate 0.0000009\n",
      "\tVal Wil 0.4710\t Val Loss 113.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  57%|████▌   | 114/200 [01:40<01:11,  1.20it/s, loss=1.4661, lr=0.000001]"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "# best_wil = 0.4252399123828323\n",
    "for epoch in range(1):\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    asrmodel.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        try:\n",
    "            if (i+1)%num_updates==0:\n",
    "                batch_bar.close()\n",
    "                train_loss = total_loss / num_updates\n",
    "                val_loss, val_wil = validate_model(asrmodel,val_loader, val_updates)\n",
    "                curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "                print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "                print(\"\\tVal Wil {:.04f}\\t Val Loss {:.04f}\".format(val_wil, val_loss))\n",
    "                wandb.log({\n",
    "                    'train_loss': train_loss,  \n",
    "                    'val_wil': val_wil, \n",
    "                    'val_loss': val_loss, \n",
    "                    'lr'        : curr_lr,\n",
    "                    'epoch'     : i//num_updates,\n",
    "                })\n",
    "                save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, epoch_model_path)\n",
    "                if val_wil <= best_wil:\n",
    "                    best_wil = val_wil\n",
    "                    save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, best_model_path)\n",
    "                    print(\"saving best model\")\n",
    "                total_loss = 0\n",
    "                batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x, y, p, lx, ly, yc = data\n",
    "            x, y, p, yc = x.to(DEVICE), y.to(DEVICE),p.to(DEVICE), yc.to(DEVICE)\n",
    "\n",
    "            with torch.cuda.amp.autocast():     \n",
    "                h, lh = asrmodel(x, y)\n",
    "                h = torch.permute(h, (0, 2, 1))\n",
    "                loss = criterion(h, yc)\n",
    "                loss = torch.sum(loss,dim=1)\n",
    "                # loss /= p\n",
    "                loss = torch.mean(loss)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "            batch_bar.update() # Update tqdm bar\n",
    "\n",
    "            # Another couple things you need for FP16. \n",
    "            scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1)\n",
    "            scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "            scaler.update() # This is something added just for FP16\n",
    "            scheduler.step()\n",
    "\n",
    "            del x, y, lx, ly, h, lh, yc, loss \n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            print(\"exception occured\")\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    val_loss, val_wil = validate_model(asrmodel,val_loader, val_updates)\n",
    "    save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
    "    if val_wil <= best_wil:\n",
    "        best_wil = val_wil\n",
    "        save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
    "        print(\"saving best model\")\n",
    "    \n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNukCVCkSl1m"
   },
   "source": [
    "# Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tF709mAAAqn",
    "outputId": "19937ab6-21a9-40d9-8d07-c12eb5c23977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 [16216, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13]\n",
      "11 [50364, 18951, 2405, 307, 1071, 1021, 39230, 294, 43120, 13, 50564]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "# t = tokenizer.encode(\"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\")\n",
    "# print(len(t),t)\n",
    "\n",
    "# # import torchaudio\n",
    "# # def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "# #     waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "# #     if sample_rate != sr:\n",
    "# #         waveform = tat.Resample(sr, sample_rate)(waveform)\n",
    "# #     return waveform\n",
    "# # audio = load_wave('/content/data/recordings/recordings/english1.mp3', sample_rate=16000)\n",
    "# # audio = whisper.pad_or_trim(audio.flatten())\n",
    "# # mel = whisper.log_mel_spectrogram(audio)\n",
    "# mel = whisper.audio.log_mel_spectrogram('/data/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_22930344.mp3',padding=N_SAMPLES)\n",
    "# mel = whisper.audio.pad_or_trim(mel, N_FRAMES).to('cuda').to(torch.float32)\n",
    "# options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
    "# m = model.decode(mel,options)\n",
    "# print(len(m.tokens),m.tokens)\n",
    "# # d = predict(model, mel)\n",
    "# # print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "m-j_XwQOG30v",
    "outputId": "f93f1b28-a0aa-4ef6-9dcb-0f7080b96e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Please call Stella. Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50364, 2555, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 11678, 50608, 50608, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 50880, 50880, 3708, 6085, 13, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51120, 51120, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 293, 321, 486, 352, 1677, 720, 10579, 51390, 51390, 412, 264, 3847, 5214, 13, 51440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "x4kZ_28EHSGx",
    "outputId": "8f6a678e-2ee0-4a6e-9a2a-2ce6274c6c2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16216, 818, 45073, 13, 220, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 220, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 220, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97UX2joWHrbZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQdYISW1oY9V"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, mel):\n",
    "  decode_options = dict()\n",
    "  decode_options[\"fp16\"] = (DEVICE == 'cuda')\n",
    "  if decode_options[\"fp16\"]:\n",
    "    dtype = torch.float16\n",
    "  else:\n",
    "    dtype = torch.float32\n",
    "  logprob_threshold = -1.0\n",
    "  no_speech_threshold = 0.6\n",
    "  decode_options[\"language\"] = \"en\"\n",
    "\n",
    "\n",
    "  content_frames = mel.shape[-1] - N_FRAMES\n",
    "\n",
    "  def decode_with_fallback(segment: torch.Tensor) -> whisper.decoding.DecodingResult:\n",
    "      kwargs = {**decode_options}\n",
    "      options = whisper.decoding.DecodingOptions(**kwargs, temperature=0.0)\n",
    "      decode_result = model.decode(segment, options)\n",
    "      return decode_result\n",
    "\n",
    "  seek = 0\n",
    "  input_stride = whisper.utils.exact_div(\n",
    "      N_FRAMES, model.dims.n_audio_ctx\n",
    "  )  # mel frames per output token: 2\n",
    "  time_precision = (\n",
    "      input_stride * HOP_LENGTH / SAMPLE_RATE\n",
    "  )  # time per output token: 0.02 (seconds)\n",
    "  all_tokens = []\n",
    "  all_segments = []\n",
    "  prompt_reset_since = 0\n",
    "\n",
    "  initial_prompt_tokens = []\n",
    "\n",
    "  def new_segment(\n",
    "      *, start: float, end: float, tokens: torch.Tensor, result: whisper.decoding.DecodingResult\n",
    "  ):\n",
    "      tokens = tokens.tolist()\n",
    "      text_tokens = [token for token in tokens if token < tokenizer.eot]\n",
    "      return {\n",
    "          \"seek\": seek,\n",
    "          \"start\": start,\n",
    "          \"end\": end,\n",
    "          \"text\": tokenizer.decode(text_tokens),\n",
    "          \"tokens\": tokens,\n",
    "          \"temperature\": result.temperature,\n",
    "          \"avg_logprob\": result.avg_logprob,\n",
    "          \"compression_ratio\": result.compression_ratio,\n",
    "          \"no_speech_prob\": result.no_speech_prob,\n",
    "      }\n",
    "\n",
    "# show the progress bar when verbose is False (if True, transcribed text will be printed)\n",
    "\n",
    "  while seek < content_frames:\n",
    "      time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
    "      mel_segment = mel[:, seek : seek + N_FRAMES]\n",
    "      segment_size = min(N_FRAMES, content_frames - seek)\n",
    "      segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
    "      mel_segment = whisper.audio.pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
    "\n",
    "      decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
    "      result: whisper.decoding.DecodingResult = decode_with_fallback(mel_segment)\n",
    "      tokens = torch.tensor(result.tokens)\n",
    "\n",
    "      if no_speech_threshold is not None:\n",
    "          # no voice activity check\n",
    "          should_skip = result.no_speech_prob > no_speech_threshold\n",
    "          if (\n",
    "              logprob_threshold is not None\n",
    "              and result.avg_logprob > logprob_threshold\n",
    "          ):\n",
    "              # don't skip if the logprob is high enough, despite the no_speech_prob\n",
    "              should_skip = False\n",
    "\n",
    "          if should_skip:\n",
    "              seek += segment_size  # fast-forward to the next segment boundary\n",
    "              continue\n",
    "\n",
    "      previous_seek = seek\n",
    "      current_segments = []\n",
    "\n",
    "      timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
    "      single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
    "\n",
    "      consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
    "      consecutive.add_(1)\n",
    "      if len(consecutive) > 0:\n",
    "          # if the output contains two consecutive timestamp tokens\n",
    "          slices = consecutive.tolist()\n",
    "          if single_timestamp_ending:\n",
    "              slices.append(len(tokens))\n",
    "\n",
    "          last_slice = 0\n",
    "          for current_slice in slices:\n",
    "              sliced_tokens = tokens[last_slice:current_slice]\n",
    "              start_timestamp_pos = (\n",
    "                  sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              end_timestamp_pos = (\n",
    "                  sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              current_segments.append(\n",
    "                  new_segment(\n",
    "                      start=time_offset + start_timestamp_pos * time_precision,\n",
    "                      end=time_offset + end_timestamp_pos * time_precision,\n",
    "                      tokens=sliced_tokens,\n",
    "                      result=result,\n",
    "                  )\n",
    "              )\n",
    "              last_slice = current_slice\n",
    "\n",
    "          if single_timestamp_ending:\n",
    "              # single timestamp at the end means no speech after the last timestamp.\n",
    "              seek += segment_size\n",
    "          else:\n",
    "              # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
    "              last_timestamp_pos = (\n",
    "                  tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              seek += last_timestamp_pos * input_stride\n",
    "      else:\n",
    "          duration = segment_duration\n",
    "          timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
    "          if (\n",
    "              len(timestamps) > 0\n",
    "              and timestamps[-1].item() != tokenizer.timestamp_begin\n",
    "          ):\n",
    "              # no consecutive timestamps but it has a timestamp; use the last one.\n",
    "              last_timestamp_pos = (\n",
    "                  timestamps[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              duration = last_timestamp_pos * time_precision\n",
    "\n",
    "          current_segments.append(\n",
    "              new_segment(\n",
    "                  start=time_offset,\n",
    "                  end=time_offset + duration,\n",
    "                  tokens=tokens,\n",
    "                  result=result,\n",
    "              )\n",
    "          )\n",
    "          seek += segment_size\n",
    "\n",
    "      if result.temperature > 0.5:\n",
    "          # do not feed the prompt tokens if a high temperature was used\n",
    "          prompt_reset_since = len(all_tokens)\n",
    "\n",
    "      # if a segment is instantaneous or does not contain text, clear it\n",
    "      for i, segment in enumerate(current_segments):\n",
    "          if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
    "              segment[\"text\"] = \"\"\n",
    "              segment[\"tokens\"] = []\n",
    "              segment[\"words\"] = []\n",
    "\n",
    "      all_segments.extend(\n",
    "          [\n",
    "              {\"id\": i, **segment}\n",
    "              for i, segment in enumerate(\n",
    "                  current_segments, start=len(all_segments)\n",
    "              )\n",
    "          ]\n",
    "      )\n",
    "      all_tokens.extend(\n",
    "          [token for segment in current_segments for token in segment[\"tokens\"]]\n",
    "      )\n",
    "\n",
    "\n",
    "  return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "GCZrqF4YNVYV"
   },
   "outputs": [],
   "source": [
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(target, output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(target, output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HcCtP1m8b00S"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LABELS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1999/859756778.py\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_map\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LABELS' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, cfx, p, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thU_GfEZFoVA"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dBbaHC9gNc"
   },
   "source": [
    "## Fetching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6gi7U8l-rTZ",
    "outputId": "0221be23-450a-48f0-c87d-bf8d9a522f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting kaggle==1.5.8\n",
      "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.8\n",
      "    Uninstalling kaggle-1.5.8:\n",
      "      Successfully uninstalled kaggle-1.5.8\n",
      "Successfully installed kaggle-1.5.8\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "mkdir: cannot create directory ‘/home/ubuntu/.kaggle’: File exists\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /home/ubuntu/.kaggle\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"UserName\",\"key\":\"Key\"}') \n",
    "#     # Put your kaggle username & key here\n",
    "with open(\"/home/ubuntu/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"sma2023\",\"key\":\"6c819f763f537a6b8bbb60cb11520dbf\"}') \n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /home/ubuntu/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/ubuntu/GMUdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCq_YfEa-nZ_",
    "outputId": "be1d2a2e-248a-4a0b-8df4-27488f4bb84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "speech-accent-archive.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rtatman/speech-accent-archive\n",
    "!unzip -qo 'speech-accent-archive.zip' -d '/home/ubuntu/GMUdata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHp_n9i7CQ2p"
   },
   "source": [
    "## Define the two metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3lSOgmCDi9q",
    "outputId": "c571de6c-c08c-43b3-ccc3-254bd197b47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.0 rapidfuzz-2.13.7\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwLmLG-kCJkj"
   },
   "outputs": [],
   "source": [
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkRPPRCX419",
    "outputId": "8282a50a-69fa-4873-a647-11160e945573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jiwer.wer(\"the cat\", \"cat the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl1z3HKBI7QM"
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/ubuntu/GMUdata/reading-passage.txt\")\n",
    "target = \"\"\n",
    "for line in f:\n",
    "  target += line\n",
    "\n",
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    # jiwer.ReduceToSingleSentence(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.SubstituteRegexes({r\"6\": r\"six\", r\"5\": r\"five\", r\"3\": r\"three\"}),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDF0PSbZNw0k"
   },
   "source": [
    "## Transcribe and recording the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pf-4J91DOMdw"
   },
   "outputs": [],
   "source": [
    "# open the speaker_all.csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ASRModel(\n",
       "   (asr): Whisper(\n",
       "     (encoder): AudioEncoder(\n",
       "       (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "       (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "       (blocks): ModuleList(\n",
       "         (0): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): TextDecoder(\n",
       "       (token_embedding): Embedding(51865, 384)\n",
       "       (blocks): ModuleList(\n",
       "         (0): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (cross_attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (cross_attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (cross_attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): ResidualAttentionBlock(\n",
       "           (attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (cross_attn): MultiHeadAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "           )\n",
       "           (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (1): GELU(approximate='none')\n",
       "             (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           )\n",
       "           (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " None,\n",
       " None,\n",
       " 68,\n",
       " 0.4234475578314993]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(best_model_path, asrmodel, metric= 'val_wil', optimizer= None, scheduler= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "015b2369aa434a74af9f62546d841984"
     ]
    },
    "id": "FkitHBn6Oba8",
    "outputId": "80de56e7-a474-4c9d-c4d7-c5b75085653f"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01717090606689453,
       "initial": 0,
       "n": 0,
       "ncols": 80,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2172,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                  | 0/2172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "speakers = pd.read_csv('../whisper_baseline/results.csv')\n",
    "speakers['new_wer'] = 1.0\n",
    "speakers['new_wil'] = 1.0\n",
    "cnt = 0\n",
    "batch_bar   = tqdm(total=len(speakers), dynamic_ncols=True, leave=False, position=0)\n",
    "for index, row in speakers.iterrows():\n",
    "  if row['file_missing?']==False:\n",
    "    file_name = row['filename']\n",
    "    transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/'+file_name+'.mp3', fp16=False, language = 'en')['text']\n",
    "    speakers.at[index,'new_wer'] = wer(transcription)\n",
    "    speakers.at[index,'new_wil'] = wil(transcription)\n",
    "    # print(\"wer\", speakers.at[index,'wer'], \"new_new\", speakers.at[index,'new_wer'] , \"wil\", speakers.at[index,'wil'], \"new_wil\", speakers.at[index,'new_wil']) \n",
    "  batch_bar.update()\n",
    "batch_bar.close()\n",
    "speakers.to_csv('is_results_06_01_best.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please call Stella, ask her to bring these things with her from the store. Six points of fresh snow peas, five sticks of blue cheese, maybe a snack for a broader barbe. We also need a small plastic snake and a big toy frag for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at a train station.\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/yiddish5.mp3', fp16=False)['text']\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please call Stella, ask her to bring these things with her from Vestor. 6 spoons of fresh snow peas, 5 teak slabs of blue sheets, and maybe a snake for her brother Bob. We also need a small plastic snake and big to eat frog from the kids. For the kids. She can scoop these things into three red bags and we will go meet her when they areGHT to enjoy the whole trip.\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/bambara5.mp3', fp16=False)['text']\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p39)",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
