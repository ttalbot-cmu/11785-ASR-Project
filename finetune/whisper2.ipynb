{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k44fylYFktg"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FeMYucCiSr0b"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\" : 1e-6,\n",
    "    \"epochs\" : 5    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yv1FeoIRbG0"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bxd0gRCbRU5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Wed Apr 26 01:28:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   22C    P8    13W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EL5wEwuZRWsi"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb --quiet\n",
    "# !pip install torchsummaryX -q\n",
    "# !pip install mutagen\n",
    "# !pip install jiwer\n",
    "# !pip install git+https://github.com/openai/whisper.git \n",
    "# # on Ubuntu or Debian\n",
    "# !sudo apt update && sudo apt install ffmpeg\n",
    "# !pip install -qqq evaluate==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['CUDA_PATH']= '/usr/local/cuda-11.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ['CUDA_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mE6sHivTH36m",
    "outputId": "175e610a-da43-46ee-b356-1ad728257611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from mutagen.mp3 import MP3\n",
    "import jiwer\n",
    "\n",
    "import wandb\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDwpf5XT_1GP"
   },
   "source": [
    "## Load the pretrained Whisper model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6dIP0MrBFks",
    "outputId": "98526568-a2cd-417a-fe52-1343352d0827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 72.1M/72.1M [00:00<00:00, 111MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PXfGq1r4ja4m"
   },
   "outputs": [],
   "source": [
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "N_FRAMES = whisper.utils.exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "FRAMES_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "TOKENS_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtdVyWyGBqH"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oAEEkVi2FiY0"
   },
   "outputs": [],
   "source": [
    "class AudioDataloader(torch.utils.data.DataLoader):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    \n",
    "    def __init__(self, partition, root, tokenizer, batch_size, shuffle): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        self.df = pd.read_csv('common_voice_'+partition+'.csv')\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_len = len(self.df)\n",
    "        self.root = root\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return round((self.df_len//self.batch_size)*0.5)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac = 1)\n",
    "        num_batches = self.__len__()\n",
    "        batch_idx = 0\n",
    "        batch_mfccs = []\n",
    "        batch_transcripts = []\n",
    "        batch_probs = []\n",
    "        for index, entry in self.df.iterrows():\n",
    "            filepath = self.root + entry['path']\n",
    "            try:\n",
    "                audio = MP3(filepath)\n",
    "                if audio.info.length < 30:\n",
    "                    transcript = tokenizer.encode(entry['sentence'])\n",
    "                    prob = entry['probs']\n",
    "                    mfcc = whisper.audio.log_mel_spectrogram(filepath,padding=N_SAMPLES)\n",
    "                    mfcc = whisper.audio.pad_or_trim(mfcc, N_FRAMES).to(DEVICE).to(dtype)\n",
    "                    batch_mfccs.append(mfcc)\n",
    "                    batch_transcripts.append(transcript)\n",
    "                    batch_probs.append(prob)\n",
    "                if len(batch_probs)== self.batch_size:\n",
    "                    batch_idx += 1\n",
    "                    yield self.collate_fn((batch_mfccs,batch_transcripts,torch.LongTensor(batch_probs)))\n",
    "                    batch_mfccs = []\n",
    "                    batch_transcripts = []\n",
    "                    batch_probs = []\n",
    "            except:\n",
    "                pass\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = [] # TODO\n",
    "        # batch of output phonemes\n",
    "        batch_transcript = [] # TODO\n",
    "        # batch_content_frame = []\n",
    "        batch_prob = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "\n",
    "        # for mfcc,transcript,content_frame, prob in batch:\n",
    "        for mfcc,transcript, prob in batch:\n",
    "        \n",
    "          batch_mfcc.append(mfcc)\n",
    "          batch_transcript.append(transcript)\n",
    "          # batch_content_frame.append(content_frame)\n",
    "          batch_prob.append(prob)\n",
    "          lengths_mfcc.append(len(mfcc))\n",
    "          lengths_transcript.append(len(transcript))\n",
    "          \n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first= True) # TODO\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first= True) # TODO\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(batch_prob), torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8EF7Hc4ikiR2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # Increase if your device can handle it\n",
    "num_updates = 200\n",
    "val_updates = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iLYG4uhNkdFD"
   },
   "outputs": [],
   "source": [
    "# Create objects for the dataset class\n",
    "root = '/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/'\n",
    "# train_data = AudioDataset('train',root, tokenizer) #TODO\n",
    "# val_data = AudioDataset('validation',root, tokenizer) # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "\n",
    "# # Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = train_data, \n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "# #     pin_memory  = True,\n",
    "#     shuffle     = True,\n",
    "#     collate_fn  = train_data.collate_fn\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = val_data,\n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "#     # pin_memory  = True,\n",
    "#     shuffle     = False,\n",
    "#     collate_fn  = val_data.collate_fn\n",
    "# )\n",
    "\n",
    "# print(\"Batch size: \", BATCH_SIZE)\n",
    "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "train_loader = AudioDataloader('train', root, tokenizer, BATCH_SIZE, shuffle = True)\n",
    "val_loader = AudioDataloader('validation', root, tokenizer, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTeyWdnTkruv"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "x, y, p, lx, ly = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "print(x,y,p,lx,ly)\n",
    "x, y, p, lx, ly = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "print(x,y,p,lx,ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5zCZ11eSFws"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BjQVLuTtIMX2"
   },
   "outputs": [],
   "source": [
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.options = whisper.decoding.DecodingOptions(fp16 = True,temperature=0.0,without_timestamps=True)\n",
    "        self.asr = model\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # print(\"x type\",type(x))\n",
    "        # print(\"x shape\",x.shape)\n",
    "        # print(\"opt type\",type(self.options))\n",
    "        # print(\"opt shape\",self.options.shape)\n",
    "        # print(\"length\",lengths_x)\n",
    "        audio_features = self.asr.encoder(x)\n",
    "        results = self.asr.decoder(y, audio_features)\n",
    "        # print(results.shape,results)\n",
    "        results = results.log_softmax(2)\n",
    "        # print(results.shape,results)\n",
    "        # out = [torch.LongTensor(result.tokens).to(DEVICE) for result in results]\n",
    "        # out = torch.nn.utils.rnn.pad_sequence(out, batch_first=True, padding_value=0.0)\n",
    "        lx = self.asr.decode(x,self.options)\n",
    "        # print(\"orig len\", len(out))\n",
    "        lx = list(map(lambda x: len(x.tokens), lx))\n",
    "        # seq_unpacked, lens_unpacked = pad_packed_sequence(out, batch_first = True)\n",
    "        # print(\"new len\",len(out))\n",
    "        lx = torch.LongTensor(lx).to(DEVICE)\n",
    "        # return out, lengths_x\n",
    "        # return seq_unpacked, lens_unpacked\n",
    "        # print(type(out))\n",
    "        # print(out.shape)\n",
    "        # out = out.double()\n",
    "        # return out, lengths_x\n",
    "        return results,lx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvLOwGjISJT1"
   },
   "source": [
    "## Criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xARobkRVLMNr"
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True) # Define CTC loss as the criterion. How would the losses be reduced?\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "optimizer =  torch.optim.SGD(model.parameters(),lr = config[\"lr\"]) # What goes in here?\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2) #TODO\n",
    "# metric = evaluate.load(\"wil\")\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbkJqcWPSQnS"
   },
   "source": [
    "## Train and validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "I3ADWv8gK1aT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def train_model(model, train_loader, optimizer):\n",
    "#     model.train()\n",
    "#     batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         x, y, p, lx, ly = data\n",
    "#         x, y, p = x.to(DEVICE), y.to(DEVICE),p.to(DEVICE)\n",
    "\n",
    "#         with torch.cuda.amp.autocast():     \n",
    "#             h,lh = model(x, y)\n",
    "#             h = torch.permute(h, (1, 0, 2))\n",
    "#             loss = criterion(h, y, ly, ly)\n",
    "            \n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         batch_bar.set_postfix(\n",
    "#             loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "#             lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "#         batch_bar.update() # Update tqdm bar\n",
    "\n",
    "#         # Another couple things you need for FP16. \n",
    "#         scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "#         scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "#         scaler.update() # This is something added just for FP16\n",
    "\n",
    "#         del x, y, lx, ly, h, lh, loss \n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# def validate_model(model, val_loader):\n",
    "\n",
    "#     model.eval()\n",
    "#     batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "#     total_loss = 0\n",
    "#     val_wil = 0\n",
    "\n",
    "#     for i, data in enumerate(val_loader):\n",
    "\n",
    "#         x, y, p, lx, ly = data\n",
    "#         x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "#         with torch.inference_mode():\n",
    "#             # # h, lh = model(x, lx)\n",
    "#             # h = model(x, y)\n",
    "#             # h = torch.permute(h,(0,2,1))\n",
    "#             # # h = torch.permute(h, (1, 0, 2))\n",
    "#             # # loss = criterion(h, y, lh, ly)\n",
    "#             # # print(y.shape)\n",
    "#             # # print(y)\n",
    "#             # loss = criterion(h,y)\n",
    "#             h,lh = model(x, y)\n",
    "#             h = torch.permute(h, (1, 0, 2))\n",
    "#             loss = criterion(h, y, ly, ly)\n",
    "#             ht = tokenizer.decode(h)\n",
    "#             hy \n",
    "#             # loss = torch.mean(loss)\n",
    "\n",
    "#         total_loss += float(loss)\n",
    "#         # val_wil += wil(h,y)\n",
    "\n",
    "#         batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
    "\n",
    "#         batch_bar.update()\n",
    "    \n",
    "#         del x, y, lx, ly, h, lh, loss\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#     batch_bar.close()\n",
    "#     total_loss = total_loss/len(val_loader)\n",
    "#     avg_wil = val_wil/len(val_loader)\n",
    "#     return total_loss, avg_wil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, val_batches):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=val_batches, dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    val_wil = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        if i == val_batches:\n",
    "            break\n",
    "        x, y, p, lx, ly = data\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h,lh = model(x, y)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y)\n",
    "            loss /= p\n",
    "            loss = sum(loss)\n",
    "            # loss = torch.mean(loss)\n",
    "        total_loss += float(loss)\n",
    "        # val_wil += wil(h,y)\n",
    "        \n",
    "        ht = model.asr.decode(x,model.options)\n",
    "        ht = list(map(lambda x: x.tokens, ht))\n",
    "        text_x,text_y = [],[]\n",
    "        for j in range(len(ht)):\n",
    "            val_wil += jiwer.wil(tokenizer.decode(ht[j]), tokenizer.decode(y[j]))/len(ht)            \n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), wil=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    avg_wil = val_wil/len(val_loader)\n",
    "    return total_loss, avg_wil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNcyukhDSUe7"
   },
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-ETu7LPQPy8W"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         metric[0]                  : metric[1], \n",
    "         'epoch'                    : epoch}, \n",
    "         path\n",
    "    )\n",
    "\n",
    "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FFUj2WUVQP3f"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "end = config[\"epochs\"]\n",
    "best_wil = 1 # if you're restarting from some checkpoint, use what you saw there.\n",
    "epoch_model_path = \"/data/home/ubuntu/11785-ASR-Project/finetune/epoch_model.checkpoint\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
    "best_model_path = \"/data/home/ubuntu/11785-ASR-Project/finetune/best_model.checkpoint\"#TODO set best model path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCgcYFPWSZOF"
   },
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NaMfgXC8RNVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshilinm\u001b[0m (\u001b[33mdeeper_learners\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"a5b7420abbe354e6d0b2f5554b97ee11f327fc92\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FjMI5MgYRN4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/ubuntu/11785-ASR-Project/finetune/wandb/run-20230417_145421-qjgvz3dv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/qjgvz3dv' target=\"_blank\">test-run</a></strong> to <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/qjgvz3dv' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune/runs/qjgvz3dv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"test-run\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = 'oigknwdr', #Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"whisper-finetune\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">first-attempt</strong> at: <a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/6xhucskr' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune/runs/6xhucskr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230415_045725-6xhucskr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq87gxHcScqH"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrmodel = ASRModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "val_loss, val_wil = validate_model(asrmodel,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start val loss 14.629981064796448 start val wil 0.5157564043184116\n"
     ]
    }
   ],
   "source": [
    "print(\"start val loss\", val_loss, \"start val wil\", val_wil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1697"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A3JIKc4mQdP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████▊| 199/200 [01:57<00:00,  1.69it/s, loss=6.1564, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.2251\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5230\t Val Loss 4.1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:05<00:00,  1.59it/s, loss=1.7096, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.6821\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5232\t Val Loss 2.7573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:11<00:00,  1.52it/s, loss=0.7922, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.4745\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5254\t Val Loss 2.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:01<00:00,  1.64it/s, loss=0.4603, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.3678\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5278\t Val Loss 1.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:13<00:00,  1.49it/s, loss=0.2939, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2936\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5292\t Val Loss 1.3635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|▏                               | 1/200 [00:00<01:40,  1.98it/s, loss=0.0017, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████▊| 199/200 [02:10<00:00,  1.53it/s, loss=1.2193, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2426\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5302\t Val Loss 1.1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:11<00:00,  1.52it/s, loss=0.5298, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2114\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5303\t Val Loss 1.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:03<00:00,  1.62it/s, loss=0.3026, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1812\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5301\t Val Loss 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:12<00:00,  1.51it/s, loss=0.2044, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1633\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5298\t Val Loss 0.7895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:13<00:00,  1.50it/s, loss=0.1484, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1482\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5293\t Val Loss 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|▏                               | 1/200 [00:00<01:36,  2.07it/s, loss=0.0005, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████▊| 199/200 [02:15<00:00,  1.47it/s, loss=0.6449, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1283\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5288\t Val Loss 0.6425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:07<00:00,  1.57it/s, loss=0.2939, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1173\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5293\t Val Loss 0.5848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:13<00:00,  1.50it/s, loss=0.1805, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1081\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5299\t Val Loss 0.5350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:14<00:00,  1.49it/s, loss=0.1224, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0978\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5303\t Val Loss 0.4913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:12<00:00,  1.51it/s, loss=0.0911, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0910\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5325\t Val Loss 0.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|▏                               | 1/200 [00:01<06:03,  1.83s/it, loss=0.0008, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████▊| 199/200 [02:15<00:00,  1.47it/s, loss=0.4182, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0832\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5329\t Val Loss 0.4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:14<00:00,  1.48it/s, loss=0.1942, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0775\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5329\t Val Loss 0.3936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:14<00:00,  1.49it/s, loss=0.1242, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0744\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5322\t Val Loss 0.3691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:12<00:00,  1.51it/s, loss=0.0876, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0700\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5325\t Val Loss 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:16<00:00,  1.46it/s, loss=0.0649, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0649\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5323\t Val Loss 0.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|▏                               | 1/200 [00:00<01:38,  2.03it/s, loss=0.0004, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████▊| 199/200 [02:13<00:00,  1.49it/s, loss=0.3033, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0604\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5324\t Val Loss 0.3119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:13<00:00,  1.50it/s, loss=0.1490, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0595\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5317\t Val Loss 0.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:19<00:00,  1.44it/s, loss=0.0941, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0564\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5317\t Val Loss 0.2839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:14<00:00,  1.48it/s, loss=0.0683, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0546\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5313\t Val Loss 0.2718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████| 200/200 [02:11<00:00,  1.52it/s, loss=0.0525, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.0524\t Learning Rate 0.0000010\n",
      "\tVal Wil 0.5321\t Val Loss 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|▏                               | 1/200 [00:00<01:42,  1.95it/s, loss=0.0002, lr=0.000001]\n",
      "                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "\n",
    "for epoch in range(0, config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        if (i+1)%num_updates==0:\n",
    "            batch_bar.close()\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            val_loss, val_wil = validate_model(asrmodel,val_loader)\n",
    "            curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "            print(\"\\tVal Wil {:.04f}\\t Val Loss {:.04f}\".format(val_wil, val_loss))\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss,  \n",
    "                'val_wil': val_wil, \n",
    "                'val_loss': val_loss, \n",
    "                'lr'        : curr_lr,\n",
    "                'epoch'     : i//num_updates,\n",
    "            })\n",
    "            save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, epoch_model_path)\n",
    "            if val_wil <= best_wil:\n",
    "                best_wil = val_wil\n",
    "                save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, best_model_path)\n",
    "            scheduler.step(val_loss)\n",
    "            total_loss = 0\n",
    "            batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "             \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, p, lx, ly = data\n",
    "        x, y, p = x.to(DEVICE), y.to(DEVICE),p.to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h,lh = asrmodel(x, y)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y)\n",
    "            loss /= p\n",
    "            loss = sum(loss)\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    val_loss, val_wil = validate_model(asrmodel,val_loader)\n",
    "    save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
    "    if val_wil <= best_wil:\n",
    "        best_wil = val_wil\n",
    "        save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
    "    # curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # train_loss              = train_model(asrmodel, train_loader, optimizer)\n",
    "    # valid_loss, val_wil  = validate_model(asrmodel, val_loader)\n",
    "    # scheduler.step(valid_loss)\n",
    "    # step_train_eval_model(asrmodel, train_loader, optimizer, 3, scheduler,val_loader)\n",
    "    # print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "    # print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(val_wil, valid_loss))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
    "#     wandb.save('epoch_model.checkpoint')\n",
    "    # print(\"Saved epoch model\")\n",
    "\n",
    "    # if val_wil <= best_wil:\n",
    "        # best_wil = val_wil\n",
    "        # save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
    "#         wandb.save('best_model.checkpoint')\n",
    "        # print(\"Saved best model\")\n",
    "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNukCVCkSl1m"
   },
   "source": [
    "# Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tF709mAAAqn",
    "outputId": "19937ab6-21a9-40d9-8d07-c12eb5c23977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 [16216, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13]\n",
      "11 [50364, 18951, 2405, 307, 1071, 1021, 39230, 294, 43120, 13, 50564]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "# t = tokenizer.encode(\"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\")\n",
    "# print(len(t),t)\n",
    "\n",
    "# # import torchaudio\n",
    "# # def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "# #     waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "# #     if sample_rate != sr:\n",
    "# #         waveform = tat.Resample(sr, sample_rate)(waveform)\n",
    "# #     return waveform\n",
    "# # audio = load_wave('/content/data/recordings/recordings/english1.mp3', sample_rate=16000)\n",
    "# # audio = whisper.pad_or_trim(audio.flatten())\n",
    "# # mel = whisper.log_mel_spectrogram(audio)\n",
    "# mel = whisper.audio.log_mel_spectrogram('/data/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_22930344.mp3',padding=N_SAMPLES)\n",
    "# mel = whisper.audio.pad_or_trim(mel, N_FRAMES).to('cuda').to(torch.float32)\n",
    "# options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
    "# m = model.decode(mel,options)\n",
    "# print(len(m.tokens),m.tokens)\n",
    "# # d = predict(model, mel)\n",
    "# # print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "m-j_XwQOG30v",
    "outputId": "f93f1b28-a0aa-4ef6-9dcb-0f7080b96e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Please call Stella. Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50364, 2555, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 11678, 50608, 50608, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 50880, 50880, 3708, 6085, 13, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51120, 51120, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 293, 321, 486, 352, 1677, 720, 10579, 51390, 51390, 412, 264, 3847, 5214, 13, 51440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "x4kZ_28EHSGx",
    "outputId": "8f6a678e-2ee0-4a6e-9a2a-2ce6274c6c2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16216, 818, 45073, 13, 220, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 220, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 220, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97UX2joWHrbZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQdYISW1oY9V"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, mel):\n",
    "  decode_options = dict()\n",
    "  decode_options[\"fp16\"] = (DEVICE == 'cuda')\n",
    "  if decode_options[\"fp16\"]:\n",
    "    dtype = torch.float16\n",
    "  else:\n",
    "    dtype = torch.float32\n",
    "  logprob_threshold = -1.0\n",
    "  no_speech_threshold = 0.6\n",
    "  decode_options[\"language\"] = \"en\"\n",
    "\n",
    "\n",
    "  content_frames = mel.shape[-1] - N_FRAMES\n",
    "\n",
    "  def decode_with_fallback(segment: torch.Tensor) -> whisper.decoding.DecodingResult:\n",
    "      kwargs = {**decode_options}\n",
    "      options = whisper.decoding.DecodingOptions(**kwargs, temperature=0.0)\n",
    "      decode_result = model.decode(segment, options)\n",
    "      return decode_result\n",
    "\n",
    "  seek = 0\n",
    "  input_stride = whisper.utils.exact_div(\n",
    "      N_FRAMES, model.dims.n_audio_ctx\n",
    "  )  # mel frames per output token: 2\n",
    "  time_precision = (\n",
    "      input_stride * HOP_LENGTH / SAMPLE_RATE\n",
    "  )  # time per output token: 0.02 (seconds)\n",
    "  all_tokens = []\n",
    "  all_segments = []\n",
    "  prompt_reset_since = 0\n",
    "\n",
    "  initial_prompt_tokens = []\n",
    "\n",
    "  def new_segment(\n",
    "      *, start: float, end: float, tokens: torch.Tensor, result: whisper.decoding.DecodingResult\n",
    "  ):\n",
    "      tokens = tokens.tolist()\n",
    "      text_tokens = [token for token in tokens if token < tokenizer.eot]\n",
    "      return {\n",
    "          \"seek\": seek,\n",
    "          \"start\": start,\n",
    "          \"end\": end,\n",
    "          \"text\": tokenizer.decode(text_tokens),\n",
    "          \"tokens\": tokens,\n",
    "          \"temperature\": result.temperature,\n",
    "          \"avg_logprob\": result.avg_logprob,\n",
    "          \"compression_ratio\": result.compression_ratio,\n",
    "          \"no_speech_prob\": result.no_speech_prob,\n",
    "      }\n",
    "\n",
    "# show the progress bar when verbose is False (if True, transcribed text will be printed)\n",
    "\n",
    "  while seek < content_frames:\n",
    "      time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
    "      mel_segment = mel[:, seek : seek + N_FRAMES]\n",
    "      segment_size = min(N_FRAMES, content_frames - seek)\n",
    "      segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
    "      mel_segment = whisper.audio.pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
    "\n",
    "      decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
    "      result: whisper.decoding.DecodingResult = decode_with_fallback(mel_segment)\n",
    "      tokens = torch.tensor(result.tokens)\n",
    "\n",
    "      if no_speech_threshold is not None:\n",
    "          # no voice activity check\n",
    "          should_skip = result.no_speech_prob > no_speech_threshold\n",
    "          if (\n",
    "              logprob_threshold is not None\n",
    "              and result.avg_logprob > logprob_threshold\n",
    "          ):\n",
    "              # don't skip if the logprob is high enough, despite the no_speech_prob\n",
    "              should_skip = False\n",
    "\n",
    "          if should_skip:\n",
    "              seek += segment_size  # fast-forward to the next segment boundary\n",
    "              continue\n",
    "\n",
    "      previous_seek = seek\n",
    "      current_segments = []\n",
    "\n",
    "      timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
    "      single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
    "\n",
    "      consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
    "      consecutive.add_(1)\n",
    "      if len(consecutive) > 0:\n",
    "          # if the output contains two consecutive timestamp tokens\n",
    "          slices = consecutive.tolist()\n",
    "          if single_timestamp_ending:\n",
    "              slices.append(len(tokens))\n",
    "\n",
    "          last_slice = 0\n",
    "          for current_slice in slices:\n",
    "              sliced_tokens = tokens[last_slice:current_slice]\n",
    "              start_timestamp_pos = (\n",
    "                  sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              end_timestamp_pos = (\n",
    "                  sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              current_segments.append(\n",
    "                  new_segment(\n",
    "                      start=time_offset + start_timestamp_pos * time_precision,\n",
    "                      end=time_offset + end_timestamp_pos * time_precision,\n",
    "                      tokens=sliced_tokens,\n",
    "                      result=result,\n",
    "                  )\n",
    "              )\n",
    "              last_slice = current_slice\n",
    "\n",
    "          if single_timestamp_ending:\n",
    "              # single timestamp at the end means no speech after the last timestamp.\n",
    "              seek += segment_size\n",
    "          else:\n",
    "              # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
    "              last_timestamp_pos = (\n",
    "                  tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              seek += last_timestamp_pos * input_stride\n",
    "      else:\n",
    "          duration = segment_duration\n",
    "          timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
    "          if (\n",
    "              len(timestamps) > 0\n",
    "              and timestamps[-1].item() != tokenizer.timestamp_begin\n",
    "          ):\n",
    "              # no consecutive timestamps but it has a timestamp; use the last one.\n",
    "              last_timestamp_pos = (\n",
    "                  timestamps[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              duration = last_timestamp_pos * time_precision\n",
    "\n",
    "          current_segments.append(\n",
    "              new_segment(\n",
    "                  start=time_offset,\n",
    "                  end=time_offset + duration,\n",
    "                  tokens=tokens,\n",
    "                  result=result,\n",
    "              )\n",
    "          )\n",
    "          seek += segment_size\n",
    "\n",
    "      if result.temperature > 0.5:\n",
    "          # do not feed the prompt tokens if a high temperature was used\n",
    "          prompt_reset_since = len(all_tokens)\n",
    "\n",
    "      # if a segment is instantaneous or does not contain text, clear it\n",
    "      for i, segment in enumerate(current_segments):\n",
    "          if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
    "              segment[\"text\"] = \"\"\n",
    "              segment[\"tokens\"] = []\n",
    "              segment[\"words\"] = []\n",
    "\n",
    "      all_segments.extend(\n",
    "          [\n",
    "              {\"id\": i, **segment}\n",
    "              for i, segment in enumerate(\n",
    "                  current_segments, start=len(all_segments)\n",
    "              )\n",
    "          ]\n",
    "      )\n",
    "      all_tokens.extend(\n",
    "          [token for segment in current_segments for token in segment[\"tokens\"]]\n",
    "      )\n",
    "\n",
    "\n",
    "  return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GCZrqF4YNVYV"
   },
   "outputs": [],
   "source": [
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(target, output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(target, output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcCtP1m8b00S"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, cfx, p, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thU_GfEZFoVA"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dBbaHC9gNc"
   },
   "source": [
    "## Fetching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6gi7U8l-rTZ",
    "outputId": "0221be23-450a-48f0-c87d-bf8d9a522f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kaggle==1.5.8\n",
      "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=2761abf8cabcf8d60f78714a24ae0b4b7d40604aa99ffa664c0530e61002775e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.13\n",
      "    Uninstalling kaggle-1.5.13:\n",
      "      Successfully uninstalled kaggle-1.5.13\n",
      "Successfully installed kaggle-1.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"UserName\",\"key\":\"Key\"}') \n",
    "#     # Put your kaggle username & key here\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"sma2023\",\"key\":\"6c819f763f537a6b8bbb60cb11520dbf\"}') \n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCq_YfEa-nZ_",
    "outputId": "be1d2a2e-248a-4a0b-8df4-27488f4bb84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading speech-accent-archive.zip to /content\n",
      " 99% 860M/865M [00:08<00:00, 65.5MB/s]\n",
      "100% 865M/865M [00:08<00:00, 106MB/s] \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rtatman/speech-accent-archive\n",
    "!unzip -qo 'speech-accent-archive.zip' -d '/content/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHp_n9i7CQ2p"
   },
   "source": [
    "## Define the two metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3lSOgmCDi9q",
    "outputId": "c571de6c-c08c-43b3-ccc3-254bd197b47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.0 rapidfuzz-2.13.7\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwLmLG-kCJkj"
   },
   "outputs": [],
   "source": [
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkRPPRCX419",
    "outputId": "8282a50a-69fa-4873-a647-11160e945573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jiwer.wer(\"the cat\", \"cat the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl1z3HKBI7QM"
   },
   "outputs": [],
   "source": [
    "f = open(\"/content/data/reading-passage.txt\")\n",
    "target = \"\"\n",
    "for line in f:\n",
    "  target += line\n",
    "\n",
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    # jiwer.ReduceToSingleSentence(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.SubstituteRegexes({r\"6\": r\"six\", r\"5\": r\"five\", r\"3\": r\"three\"}),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDF0PSbZNw0k"
   },
   "source": [
    "## Transcribe and recording the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf-4J91DOMdw"
   },
   "outputs": [],
   "source": [
    "# open the speaker_all.csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "015b2369aa434a74af9f62546d841984"
     ]
    },
    "id": "FkitHBn6Oba8",
    "outputId": "80de56e7-a474-4c9d-c4d7-c5b75085653f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015b2369aa434a74af9f62546d841984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speakers = pd.read_csv('/content/data/speakers_all.csv')\n",
    "speakers['wer'] = 1.0\n",
    "speakers['wil'] = 1.0\n",
    "cnt = 0\n",
    "batch_bar   = tqdm(total=len(speakers), dynamic_ncols=True, leave=False, position=0)\n",
    "for index, row in speakers.iterrows():\n",
    "  if row['file_missing?']==False:\n",
    "    file_name = row['filename']\n",
    "    transcription = whisper.transcribe(model = model, audio = '/content/data/recordings/recordings/'+file_name+'.mp3', fp16=False)['text']\n",
    "    speakers.at[index,'wer'] = wer(transcription)\n",
    "    speakers.at[index,'wil'] = wil(transcription)\n",
    "  batch_bar.update()\n",
    "batch_bar.close()\n",
    "speakers.to_csv('results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p39)",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
