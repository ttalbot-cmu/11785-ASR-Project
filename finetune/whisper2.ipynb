{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k44fylYFktg"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FeMYucCiSr0b"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\" : 1e-7  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yv1FeoIRbG0"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bxd0gRCbRU5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Wed Apr 26 16:54:04 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   22C    P8    13W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EL5wEwuZRWsi"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb --quiet\n",
    "# !pip install torchsummaryX -q\n",
    "# !pip install mutagen\n",
    "# !pip install jiwer\n",
    "# !pip install git+https://github.com/openai/whisper.git \n",
    "# # on Ubuntu or Debian\n",
    "# !sudo apt update && sudo apt install ffmpeg\n",
    "# !pip install -qqq evaluate==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ['CUDA_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mE6sHivTH36m",
    "outputId": "175e610a-da43-46ee-b356-1ad728257611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from mutagen.mp3 import MP3\n",
    "import jiwer\n",
    "\n",
    "import wandb\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDwpf5XT_1GP"
   },
   "source": [
    "## Load the pretrained Whisper model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6dIP0MrBFks",
    "outputId": "98526568-a2cd-417a-fe52-1343352d0827"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PXfGq1r4ja4m"
   },
   "outputs": [],
   "source": [
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "N_FRAMES = whisper.utils.exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "FRAMES_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "TOKENS_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtdVyWyGBqH"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8EF7Hc4ikiR2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # Increase if your device can handle it\n",
    "num_updates = 200\n",
    "val_updates = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oAEEkVi2FiY0"
   },
   "outputs": [],
   "source": [
    "class AudioDataloader(torch.utils.data.DataLoader):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    \n",
    "    def __init__(self, partition, root, tokenizer, batch_size, shuffle): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        self.df = pd.read_csv('common_voice_'+partition+'.csv')\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_len = len(self.df)\n",
    "        self.root = root\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return round((self.df_len//self.batch_size)*0.95)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac = 1)\n",
    "        num_batches = self.__len__()\n",
    "        batch_idx = 0\n",
    "        batch_mfccs = []\n",
    "        batch_transcripts = []\n",
    "        batch_probs = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "        for index, entry in self.df.iterrows():\n",
    "            filepath = self.root + entry['path']\n",
    "            audio = MP3(filepath)\n",
    "            if audio.info.length < 30:\n",
    "                transcript = torch.tensor(tokenizer.encode(entry['sentence']))\n",
    "                prob = entry['probs']\n",
    "                mfcc = whisper.audio.log_mel_spectrogram(filepath,padding=N_SAMPLES)\n",
    "                mfcc = whisper.audio.pad_or_trim(mfcc, N_FRAMES).to(DEVICE).to(dtype)\n",
    "                batch_mfccs.append(mfcc)\n",
    "                lengths_mfcc.append(len(mfcc))\n",
    "                batch_transcripts.append(transcript)\n",
    "                lengths_transcript.append(len(transcript))\n",
    "                batch_probs.append(prob)\n",
    "            if len(batch_probs)== self.batch_size:\n",
    "                batch_idx += 1\n",
    "                batch_mfcc_pad = pad_sequence(batch_mfccs, batch_first= True) # TODO\n",
    "                # max_label_len = max(lengths_transcript)\n",
    "                # batch_transcript_pad = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(batch_transcripts, lengths_transcript)]\n",
    "                batch_transcript_pad = pad_sequence(batch_transcripts, batch_first= True) # TODO\n",
    "                batch_transcript_pad_ent = pad_sequence(batch_transcripts, batch_first= True, padding_value = -100) # TODO\n",
    "                yield batch_mfcc_pad, batch_transcript_pad, torch.tensor(batch_probs), torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript),batch_transcript_pad_ent\n",
    "                batch_mfccs = []\n",
    "                batch_transcripts = []\n",
    "                batch_probs = []\n",
    "                lengths_mfcc = []\n",
    "                lengths_transcript = []\n",
    "        self.length = len(self.mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iLYG4uhNkdFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches 59530\n",
      "val batches 14882\n"
     ]
    }
   ],
   "source": [
    "# Create objects for the dataset class\n",
    "root = '/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/'\n",
    "# train_data = AudioDataset('train',root, tokenizer) #TODO\n",
    "# val_data = AudioDataset('validation',root, tokenizer) # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "\n",
    "# # Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = train_data, \n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "# #     pin_memory  = True,\n",
    "#     shuffle     = True,\n",
    "#     collate_fn  = train_data.collate_fn\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = val_data,\n",
    "#     num_workers = 0,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "#     # pin_memory  = True,\n",
    "#     shuffle     = False,\n",
    "#     collate_fn  = val_data.collate_fn\n",
    "# )\n",
    "\n",
    "# print(\"Batch size: \", BATCH_SIZE)\n",
    "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "train_loader = AudioDataloader('train', root, tokenizer, BATCH_SIZE, shuffle = True)\n",
    "val_loader = AudioDataloader('validation', root, tokenizer, BATCH_SIZE, shuffle = True)\n",
    "print(\"train batches\", len(train_loader))\n",
    "print(\"val batches\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vTeyWdnTkruv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80, 3000]) torch.Size([10, 20]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 80, 3000]) torch.Size([10, 17]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "x, y, p, lx, ly, yc = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "# print(x,y,p,lx,ly)\n",
    "x, y, p, lx, ly, yc = next(iter(train_loader))\n",
    "print(x.shape, y.shape, p.shape, lx.shape, ly.shape)\n",
    "# print(x,y,p,lx,ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5zCZ11eSFws"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BjQVLuTtIMX2"
   },
   "outputs": [],
   "source": [
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.options = whisper.decoding.DecodingOptions(fp16 = True,temperature=0.0,without_timestamps=True)\n",
    "        self.asr = model\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # print(\"x type\",type(x))\n",
    "        # print(\"x shape\",x.shape)\n",
    "        # print(\"opt type\",type(self.options))\n",
    "        # print(\"opt shape\",self.options.shape)\n",
    "        # print(\"length\",lengths_x)\n",
    "        audio_features = self.asr.encoder(x)\n",
    "        results = self.asr.decoder(y, audio_features)\n",
    "        # print(results.shape,results)\n",
    "        results = results.log_softmax(2)\n",
    "        # print(results.shape,results)\n",
    "        # out = [torch.LongTensor(result.tokens).to(DEVICE) for result in results]\n",
    "        # out = torch.nn.utils.rnn.pad_sequence(out, batch_first=True, padding_value=0.0)\n",
    "        lx = self.asr.decode(x,self.options)\n",
    "        # print(\"orig len\", len(out))\n",
    "        lx = list(map(lambda x: len(x.tokens), lx))\n",
    "        # seq_unpacked, lens_unpacked = pad_packed_sequence(out, batch_first = True)\n",
    "        # print(\"new len\",len(out))\n",
    "        lx = torch.LongTensor(lx).to(DEVICE)\n",
    "        # return out, lengths_x\n",
    "        # return seq_unpacked, lens_unpacked\n",
    "        # print(type(out))\n",
    "        # print(out.shape)\n",
    "        # out = out.double()\n",
    "        # return out, lengths_x\n",
    "        return results,lx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvLOwGjISJT1"
   },
   "source": [
    "## Criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xARobkRVLMNr"
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True) # Define CTC loss as the criterion. How would the losses be reduced?\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "optimizer =  torch.optim.SGD(model.parameters(),lr = config[\"lr\"]) # What goes in here?\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0, total_iters=len(train_loader), last_epoch=- 1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2) #TODO\n",
    "# metric = evaluate.load(\"wil\")\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbkJqcWPSQnS"
   },
   "source": [
    "## Train and validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, val_batches):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=val_batches, dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    val_wil = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        if i == val_batches:\n",
    "            break\n",
    "        x, y, p, lx, ly, yc = data\n",
    "        x, y, p, yc = x.to(DEVICE), y.to(DEVICE), p.to(DEVICE), yc.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h,lh = model(x, y)\n",
    "            h = torch.permute(h, (0, 2, 1))\n",
    "            loss = criterion(h, yc)\n",
    "            loss = torch.sum(loss,dim=1)\n",
    "            loss = torch.mean(loss)\n",
    "            # loss = torch.mean(loss)\n",
    "        total_loss += float(loss)\n",
    "        # val_wil += wil(h,y)\n",
    "        \n",
    "        ht = model.asr.decode(x,model.options)\n",
    "        ht = list(map(lambda x: x.tokens, ht))\n",
    "        text_x,text_y = [],[]\n",
    "        for j in range(len(ht)):\n",
    "            val_wil += jiwer.wil(tokenizer.decode(ht[j]), tokenizer.decode(y[j]))/len(ht)            \n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), wil=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, yc, ht, text_x, text_y, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/ val_batches\n",
    "    avg_wil = val_wil/ val_batches\n",
    "    return total_loss, avg_wil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNcyukhDSUe7"
   },
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-ETu7LPQPy8W"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         metric[0]                  : metric[1], \n",
    "         'epoch'                    : epoch}, \n",
    "         path\n",
    "    )\n",
    "\n",
    "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FFUj2WUVQP3f"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "best_wil = 1 # if you're restarting from some checkpoint, use what you saw there.\n",
    "epoch_model_path = \"/home/ubuntu/11785-ASR-Project/finetune/is_epoch_model.checkpoint\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
    "best_model_path = \"/home/ubuntu/11785-ASR-Project/finetune/is_best_model.checkpoint\"#TODO set best model path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCgcYFPWSZOF",
    "tags": []
   },
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NaMfgXC8RNVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshilinm\u001b[0m (\u001b[33mdeeper_learners\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"a5b7420abbe354e6d0b2f5554b97ee11f327fc92\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FjMI5MgYRN4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/11785-ASR-Project/finetune/wandb/run-20230426_165812-sfkli63g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/sfkli63g' target=\"_blank\">is-test-run-clip</a></strong> to <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeper_learners/whisper-finetune' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/sfkli63g' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune/runs/sfkli63g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"is-test-run-clip\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = 'oigknwdr', #Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"whisper-finetune\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▁▂▃▅▆▇█</td></tr><tr><td>lr</td><td>█▇▇▅▅▄▃▂▂▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▄▂▁▁▂▁</td></tr><tr><td>val_loss</td><td>█▇▇▅▄▃▂▂▁▁</td></tr><tr><td>val_wil</td><td>▂▃▅▂█▃▆▅▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>2616.80546</td></tr><tr><td>val_loss</td><td>97.57123</td></tr><tr><td>val_wil</td><td>0.45341</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">is-test-run-clip</strong> at: <a href='https://wandb.ai/deeper_learners/whisper-finetune/runs/sfkli63g' target=\"_blank\">https://wandb.ai/deeper_learners/whisper-finetune/runs/sfkli63g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230426_165812-sfkli63g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq87gxHcScqH"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrmodel = ASRModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start val loss 189.19674072265624 start val wil 0.460514406962717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "val_loss, val_wil = validate_model(asrmodel,val_loader, 10)\n",
    "print(\"start val loss\", val_loss, \"start val wil\", val_wil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A3JIKc4mQdP7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  91%|████████████████████████▌  | 182/200 [18:39<01:50,  6.15s/it, loss=1063.3277, lr=0.000000]\n",
      "Train: 100%|██████████████████████████▊| 199/200 [07:33<00:02,  2.28s/it, loss=3080.3175, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 3064.9159\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4332\t Val Loss 146.4547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|███████████████████████████| 200/200 [07:25<00:00,  2.23s/it, loss=2313.3193, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 4615.0719\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4717\t Val Loss 133.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|███████████████████████████| 200/200 [07:22<00:00,  2.21s/it, loss=1042.7108, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 3122.9190\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4412\t Val Loss 119.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████| 200/200 [07:24<00:00,  2.22s/it, loss=643.0741, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 2569.0811\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4578\t Val Loss 114.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████| 200/200 [07:30<00:00,  2.25s/it, loss=553.5398, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 2764.9311\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4510\t Val Loss 110.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████| 200/200 [07:34<00:00,  2.27s/it, loss=521.0464, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 3123.6732\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4277\t Val Loss 101.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████| 200/200 [07:23<00:00,  2.22s/it, loss=374.0966, lr=0.000000]\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 2616.8055\t Learning Rate 0.0000001\n",
      "\tVal Wil 0.4534\t Val Loss 97.5712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  42%|████████████▎                | 85/200 [03:06<04:10,  2.18s/it, loss=174.4058, lr=0.000000]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1840) must match the size of tensor b (448) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8328/1428418310.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masrmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8328/3946526699.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(\"length\",lengths_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(results.shape,results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkv_cache\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         x = (\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1840) must match the size of tensor b (448) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "    asrmodel.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        if (i+1)%num_updates==0:\n",
    "            batch_bar.close()\n",
    "            train_loss = total_loss / num_updates\n",
    "            val_loss, val_wil = validate_model(asrmodel,val_loader, val_updates)\n",
    "            curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "            print(\"\\tVal Wil {:.04f}\\t Val Loss {:.04f}\".format(val_wil, val_loss))\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss,  \n",
    "                'val_wil': val_wil, \n",
    "                'val_loss': val_loss, \n",
    "                'lr'        : curr_lr,\n",
    "                'epoch'     : i//num_updates,\n",
    "            })\n",
    "            save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, epoch_model_path)\n",
    "            if val_wil <= best_wil:\n",
    "                best_wil = val_wil\n",
    "                save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], i//num_updates, best_model_path)\n",
    "            total_loss = 0\n",
    "            batch_bar = tqdm(total=num_updates, dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "             \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, p, lx, ly, yc = data\n",
    "        x, y, p, yc = x.to(DEVICE), y.to(DEVICE),p.to(DEVICE), yc.to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = asrmodel(x, y)\n",
    "            h = torch.permute(h, (0, 2, 1))\n",
    "            loss = criterion(h, yc)\n",
    "            loss = torch.sum(loss,dim=1)\n",
    "            loss /= p\n",
    "            loss = torch.mean(loss)\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1)\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "        scheduler.step()\n",
    "        \n",
    "        del x, y, lx, ly, h, lh, yc, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    val_loss, val_wil = validate_model(asrmodel,val_loader, val_updates)\n",
    "    save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
    "    if val_wil <= best_wil:\n",
    "        best_wil = val_wil\n",
    "        save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
    "        print(\"saving best model\")\n",
    "    \n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNukCVCkSl1m"
   },
   "source": [
    "# Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tF709mAAAqn",
    "outputId": "19937ab6-21a9-40d9-8d07-c12eb5c23977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 [16216, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13]\n",
      "11 [50364, 18951, 2405, 307, 1071, 1021, 39230, 294, 43120, 13, 50564]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "# t = tokenizer.encode(\"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\")\n",
    "# print(len(t),t)\n",
    "\n",
    "# # import torchaudio\n",
    "# # def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "# #     waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "# #     if sample_rate != sr:\n",
    "# #         waveform = tat.Resample(sr, sample_rate)(waveform)\n",
    "# #     return waveform\n",
    "# # audio = load_wave('/content/data/recordings/recordings/english1.mp3', sample_rate=16000)\n",
    "# # audio = whisper.pad_or_trim(audio.flatten())\n",
    "# # mel = whisper.log_mel_spectrogram(audio)\n",
    "# mel = whisper.audio.log_mel_spectrogram('/data/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/common_voice_en_22930344.mp3',padding=N_SAMPLES)\n",
    "# mel = whisper.audio.pad_or_trim(mel, N_FRAMES).to('cuda').to(torch.float32)\n",
    "# options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
    "# m = model.decode(mel,options)\n",
    "# print(len(m.tokens),m.tokens)\n",
    "# # d = predict(model, mel)\n",
    "# # print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "m-j_XwQOG30v",
    "outputId": "f93f1b28-a0aa-4ef6-9dcb-0f7080b96e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Please call Stella. Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50364, 2555, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 11678, 50608, 50608, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 50880, 50880, 3708, 6085, 13, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51120, 51120, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 293, 321, 486, 352, 1677, 720, 10579, 51390, 51390, 412, 264, 3847, 5214, 13, 51440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "x4kZ_28EHSGx",
    "outputId": "8f6a678e-2ee0-4a6e-9a2a-2ce6274c6c2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16216, 818, 45073, 13, 220, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 220, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 220, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97UX2joWHrbZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQdYISW1oY9V"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, mel):\n",
    "  decode_options = dict()\n",
    "  decode_options[\"fp16\"] = (DEVICE == 'cuda')\n",
    "  if decode_options[\"fp16\"]:\n",
    "    dtype = torch.float16\n",
    "  else:\n",
    "    dtype = torch.float32\n",
    "  logprob_threshold = -1.0\n",
    "  no_speech_threshold = 0.6\n",
    "  decode_options[\"language\"] = \"en\"\n",
    "\n",
    "\n",
    "  content_frames = mel.shape[-1] - N_FRAMES\n",
    "\n",
    "  def decode_with_fallback(segment: torch.Tensor) -> whisper.decoding.DecodingResult:\n",
    "      kwargs = {**decode_options}\n",
    "      options = whisper.decoding.DecodingOptions(**kwargs, temperature=0.0)\n",
    "      decode_result = model.decode(segment, options)\n",
    "      return decode_result\n",
    "\n",
    "  seek = 0\n",
    "  input_stride = whisper.utils.exact_div(\n",
    "      N_FRAMES, model.dims.n_audio_ctx\n",
    "  )  # mel frames per output token: 2\n",
    "  time_precision = (\n",
    "      input_stride * HOP_LENGTH / SAMPLE_RATE\n",
    "  )  # time per output token: 0.02 (seconds)\n",
    "  all_tokens = []\n",
    "  all_segments = []\n",
    "  prompt_reset_since = 0\n",
    "\n",
    "  initial_prompt_tokens = []\n",
    "\n",
    "  def new_segment(\n",
    "      *, start: float, end: float, tokens: torch.Tensor, result: whisper.decoding.DecodingResult\n",
    "  ):\n",
    "      tokens = tokens.tolist()\n",
    "      text_tokens = [token for token in tokens if token < tokenizer.eot]\n",
    "      return {\n",
    "          \"seek\": seek,\n",
    "          \"start\": start,\n",
    "          \"end\": end,\n",
    "          \"text\": tokenizer.decode(text_tokens),\n",
    "          \"tokens\": tokens,\n",
    "          \"temperature\": result.temperature,\n",
    "          \"avg_logprob\": result.avg_logprob,\n",
    "          \"compression_ratio\": result.compression_ratio,\n",
    "          \"no_speech_prob\": result.no_speech_prob,\n",
    "      }\n",
    "\n",
    "# show the progress bar when verbose is False (if True, transcribed text will be printed)\n",
    "\n",
    "  while seek < content_frames:\n",
    "      time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
    "      mel_segment = mel[:, seek : seek + N_FRAMES]\n",
    "      segment_size = min(N_FRAMES, content_frames - seek)\n",
    "      segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
    "      mel_segment = whisper.audio.pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
    "\n",
    "      decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
    "      result: whisper.decoding.DecodingResult = decode_with_fallback(mel_segment)\n",
    "      tokens = torch.tensor(result.tokens)\n",
    "\n",
    "      if no_speech_threshold is not None:\n",
    "          # no voice activity check\n",
    "          should_skip = result.no_speech_prob > no_speech_threshold\n",
    "          if (\n",
    "              logprob_threshold is not None\n",
    "              and result.avg_logprob > logprob_threshold\n",
    "          ):\n",
    "              # don't skip if the logprob is high enough, despite the no_speech_prob\n",
    "              should_skip = False\n",
    "\n",
    "          if should_skip:\n",
    "              seek += segment_size  # fast-forward to the next segment boundary\n",
    "              continue\n",
    "\n",
    "      previous_seek = seek\n",
    "      current_segments = []\n",
    "\n",
    "      timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
    "      single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
    "\n",
    "      consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
    "      consecutive.add_(1)\n",
    "      if len(consecutive) > 0:\n",
    "          # if the output contains two consecutive timestamp tokens\n",
    "          slices = consecutive.tolist()\n",
    "          if single_timestamp_ending:\n",
    "              slices.append(len(tokens))\n",
    "\n",
    "          last_slice = 0\n",
    "          for current_slice in slices:\n",
    "              sliced_tokens = tokens[last_slice:current_slice]\n",
    "              start_timestamp_pos = (\n",
    "                  sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              end_timestamp_pos = (\n",
    "                  sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              current_segments.append(\n",
    "                  new_segment(\n",
    "                      start=time_offset + start_timestamp_pos * time_precision,\n",
    "                      end=time_offset + end_timestamp_pos * time_precision,\n",
    "                      tokens=sliced_tokens,\n",
    "                      result=result,\n",
    "                  )\n",
    "              )\n",
    "              last_slice = current_slice\n",
    "\n",
    "          if single_timestamp_ending:\n",
    "              # single timestamp at the end means no speech after the last timestamp.\n",
    "              seek += segment_size\n",
    "          else:\n",
    "              # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
    "              last_timestamp_pos = (\n",
    "                  tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              seek += last_timestamp_pos * input_stride\n",
    "      else:\n",
    "          duration = segment_duration\n",
    "          timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
    "          if (\n",
    "              len(timestamps) > 0\n",
    "              and timestamps[-1].item() != tokenizer.timestamp_begin\n",
    "          ):\n",
    "              # no consecutive timestamps but it has a timestamp; use the last one.\n",
    "              last_timestamp_pos = (\n",
    "                  timestamps[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              duration = last_timestamp_pos * time_precision\n",
    "\n",
    "          current_segments.append(\n",
    "              new_segment(\n",
    "                  start=time_offset,\n",
    "                  end=time_offset + duration,\n",
    "                  tokens=tokens,\n",
    "                  result=result,\n",
    "              )\n",
    "          )\n",
    "          seek += segment_size\n",
    "\n",
    "      if result.temperature > 0.5:\n",
    "          # do not feed the prompt tokens if a high temperature was used\n",
    "          prompt_reset_since = len(all_tokens)\n",
    "\n",
    "      # if a segment is instantaneous or does not contain text, clear it\n",
    "      for i, segment in enumerate(current_segments):\n",
    "          if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
    "              segment[\"text\"] = \"\"\n",
    "              segment[\"tokens\"] = []\n",
    "              segment[\"words\"] = []\n",
    "\n",
    "      all_segments.extend(\n",
    "          [\n",
    "              {\"id\": i, **segment}\n",
    "              for i, segment in enumerate(\n",
    "                  current_segments, start=len(all_segments)\n",
    "              )\n",
    "          ]\n",
    "      )\n",
    "      all_tokens.extend(\n",
    "          [token for segment in current_segments for token in segment[\"tokens\"]]\n",
    "      )\n",
    "\n",
    "\n",
    "  return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GCZrqF4YNVYV"
   },
   "outputs": [],
   "source": [
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(target, output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(target, output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcCtP1m8b00S"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, cfx, p, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thU_GfEZFoVA"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dBbaHC9gNc"
   },
   "source": [
    "## Fetching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6gi7U8l-rTZ",
    "outputId": "0221be23-450a-48f0-c87d-bf8d9a522f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting kaggle==1.5.8\n",
      "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.8\n",
      "    Uninstalling kaggle-1.5.8:\n",
      "      Successfully uninstalled kaggle-1.5.8\n",
      "Successfully installed kaggle-1.5.8\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "mkdir: cannot create directory ‘/home/ubuntu/.kaggle’: File exists\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /home/ubuntu/.kaggle\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"UserName\",\"key\":\"Key\"}') \n",
    "#     # Put your kaggle username & key here\n",
    "with open(\"/home/ubuntu/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"sma2023\",\"key\":\"6c819f763f537a6b8bbb60cb11520dbf\"}') \n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /home/ubuntu/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/ubuntu/GMUdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCq_YfEa-nZ_",
    "outputId": "be1d2a2e-248a-4a0b-8df4-27488f4bb84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "speech-accent-archive.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rtatman/speech-accent-archive\n",
    "!unzip -qo 'speech-accent-archive.zip' -d '/home/ubuntu/GMUdata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHp_n9i7CQ2p"
   },
   "source": [
    "## Define the two metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3lSOgmCDi9q",
    "outputId": "c571de6c-c08c-43b3-ccc3-254bd197b47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.0 rapidfuzz-2.13.7\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwLmLG-kCJkj"
   },
   "outputs": [],
   "source": [
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkRPPRCX419",
    "outputId": "8282a50a-69fa-4873-a647-11160e945573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jiwer.wer(\"the cat\", \"cat the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xl1z3HKBI7QM"
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/ubuntu/GMUdata/reading-passage.txt\")\n",
    "target = \"\"\n",
    "for line in f:\n",
    "  target += line\n",
    "\n",
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    # jiwer.ReduceToSingleSentence(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.SubstituteRegexes({r\"6\": r\"six\", r\"5\": r\"five\", r\"3\": r\"three\"}),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDF0PSbZNw0k"
   },
   "source": [
    "## Transcribe and recording the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pf-4J91DOMdw"
   },
   "outputs": [],
   "source": [
    "# open the speaker_all.csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "015b2369aa434a74af9f62546d841984"
     ]
    },
    "id": "FkitHBn6Oba8",
    "outputId": "80de56e7-a474-4c9d-c4d7-c5b75085653f"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01658940315246582,
       "initial": 0,
       "n": 0,
       "ncols": 102,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2172,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                        | 0/2172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  42%|██████████▏             | 85/200 [3:15:46<4:24:51, 138.19s/it, loss=174.4058, lr=0.000000]\n"
     ]
    }
   ],
   "source": [
    "speakers = pd.read_csv('../whisper_baseline/results.csv')\n",
    "speakers['new_wer'] = 1.0\n",
    "speakers['new_wil'] = 1.0\n",
    "cnt = 0\n",
    "batch_bar   = tqdm(total=len(speakers), dynamic_ncols=True, leave=False, position=0)\n",
    "for index, row in speakers.iterrows():\n",
    "  if row['file_missing?']==False:\n",
    "    file_name = row['filename']\n",
    "    transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/'+file_name+'.mp3', fp16=False)['text']\n",
    "    speakers.at[index,'new_wer'] = wer(transcription)\n",
    "    speakers.at[index,'new_wil'] = wil(transcription)\n",
    "    # print(\"wer\", speakers.at[index,'wer'], \"new_new\", speakers.at[index,'new_wer'] , \"wil\", speakers.at[index,'wil'], \"new_wil\", speakers.at[index,'new_wil']) \n",
    "  batch_bar.update()\n",
    "batch_bar.close()\n",
    "speakers.to_csv('is_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please call Stella, ask her to bring these things with her from the store. Six points of fresh snow peas, five sticks of blue cheese, maybe a snack for a broader barbe. We also need a small plastic snake and a big toy frag for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at a train station.\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/yiddish5.mp3', fp16=False)['text']\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please call Stella, ask her to bring these things with her from Vestor. 6 spoons of fresh snow peas, 5 teak slabs of blue sheets, and maybe a snake for her brother Bob. We also need a small plastic snake and big to eat frog from the kids. For the kids. She can scoop these things into three red bags and we will go meet her when they areGHT to enjoy the whole trip.\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper.transcribe(model = model, audio = '/home/ubuntu/GMUdata/recordings/recordings/bambara5.mp3', fp16=False)['text']\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p39)",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
