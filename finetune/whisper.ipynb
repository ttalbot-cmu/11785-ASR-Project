{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning"
      ],
      "metadata": {
        "id": "7k44fylYFktg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"lr\" : 1e-5,\n",
        "    \"epochs\" : 2    \n",
        "    }"
      ],
      "metadata": {
        "id": "FeMYucCiSr0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up"
      ],
      "metadata": {
        "id": "7yv1FeoIRbG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ],
      "metadata": {
        "id": "Bxd0gRCbRU5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install torchsummaryX -q\n",
        "!pip install mutagen\n",
        "!pip install jiwer\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "# on Ubuntu or Debian\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "EL5wEwuZRWsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from mutagen.mp3 import MP3\n",
        "import jiwer\n",
        "\n",
        "import wandb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    dtype = torch.float16\n",
        "else:\n",
        "    dtype = torch.float32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE6sHivTH36m",
        "outputId": "175e610a-da43-46ee-b356-1ad728257611"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hard-coded audio hyperparameters\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 400\n",
        "N_MELS = 80\n",
        "HOP_LENGTH = 160\n",
        "CHUNK_LENGTH = 30\n",
        "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
        "N_FRAMES = whisper.utils.exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
        "\n",
        "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
        "FRAMES_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
        "TOKENS_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
      ],
      "metadata": {
        "id": "PXfGq1r4ja4m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDwpf5XT_1GP"
      },
      "source": [
        "## Load the pretrained Whisper model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V6dIP0MrBFks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98526568-a2cd-417a-fe52-1343352d0827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 72.1M/72.1M [00:04<00:00, 15.6MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"tiny\")\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "sdtdVyWyGBqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    \n",
        "    def __init__(self, partition, root, tokenizer): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "        df = pd.read_csv('common_voice_'+partition+'.csv')\n",
        "        # self.length = len(df)\n",
        "        \n",
        "        self.mfccs = []\n",
        "        self.transcripts = []\n",
        "        self.probs = []\n",
        "        # self.content_frames = []\n",
        "        for index, entry in df.iterrows():\n",
        "          filepath = root + entry['path']\n",
        "          audio = MP3(filepath)\n",
        "          if audio.info.length < 30:\n",
        "              transcript = tokenizer.encode(entry['sentence'])\n",
        "              prob = entry['probs']\n",
        "              mfcc = whisper.audio.log_mel_spectrogram(filepath,padding=N_SAMPLES)\n",
        "              mfcc = whisper.audio.pad_or_trim(mfcc, N_FRAMES).to(DEVICE).to(dtype)\n",
        "              # content_frame = mfcc.shape[-1] - N_FRAMES\n",
        "              mfcc = (mfcc-np.average(mfcc,axis=0))/np.std(mfcc,axis=0)\n",
        "              self.mfccs.append(mfcc)\n",
        "              self.transcripts.append(transcript)\n",
        "              self.probs.append(prob)\n",
        "              # self.content_frames.append(content_frame)\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        \n",
        "        \n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript = torch.LongTensor(self.transcripts[ind])\n",
        "        prob = torch.FloatTensor(self.probs[ind])\n",
        "        # content_frame = torch.LongTensor(self.content_frames[ind])\n",
        "        # return mfcc, transcript, content_frame, prob\n",
        "        return mfcc, transcript, prob\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [] # TODO\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [] # TODO\n",
        "        # batch_content_frame = []\n",
        "        batch_prob = []\n",
        "        lengths_mfcc = []\n",
        "        lengths_transcript = []\n",
        "\n",
        "        # for mfcc,transcript,content_frame, prob in batch:\n",
        "        for mfcc,transcript, prob in batch:\n",
        "        \n",
        "          batch_mfcc.append(mfcc)\n",
        "          batch_transcript.append(transcript)\n",
        "          # batch_content_frame.append(content_frame)\n",
        "          batch_prob.append(prob)\n",
        "          lengths_mfcc.append(len(mfcc))\n",
        "          lengths_transcript.append(len(transcript))\n",
        "          \n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first= True) # TODO\n",
        "        # lengths_mfcc = batch_mfcc_pad.shape[1] # TODO \n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first= True) # TODO\n",
        "        # lengths_transcript = batch_transcript_pad.shape[1] # TODO\n",
        "        \n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        # return batch_mfcc_pad, batch_transcript_pad, batch_content_frame,  batch_prob, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "        return batch_mfcc_pad, batch_transcript_pad, batch_prob, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n"
      ],
      "metadata": {
        "id": "oAEEkVi2FiY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 # Increase if your device can handle it"
      ],
      "metadata": {
        "id": "8EF7Hc4ikiR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset() #TODO\n",
        "val_data = AudioDataset() # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn  = val_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ],
      "metadata": {
        "id": "iLYG4uhNkdFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly, p = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape, p.shape)\n",
        "    break "
      ],
      "metadata": {
        "id": "vTeyWdnTkruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "d5zCZ11eSFws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
        "        self.asr = model\n",
        "        \n",
        "    \n",
        "    def forward(self, x, lengths_x):\n",
        "        out = self.asr.decode(x,self.options)\n",
        "\n",
        "        return out, lengths_x"
      ],
      "metadata": {
        "id": "BjQVLuTtIMX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criterion, optimizer, scheduler"
      ],
      "metadata": {
        "id": "SvLOwGjISJT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True) # Define CTC loss as the criterion. How would the losses be reduced?\n",
        "\n",
        "optimizer =  torch.optim.SGD(model.parameters(),lr = config[\"lr\"]) # What goes in here?\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3) #TODO\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "xARobkRVLMNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validate functions"
      ],
      "metadata": {
        "id": "sbkJqcWPSQnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly, p = data\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            # h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "            loss = loss/p\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    val_wil = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            # h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        val_wil += wil(h,y)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    avg_wil = val_wil/len(val_loader)\n",
        "    return total_loss, avg_wil"
      ],
      "metadata": {
        "id": "I3ADWv8gK1aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training setup"
      ],
      "metadata": {
        "id": "SNcyukhDSUe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ],
      "metadata": {
        "id": "-ETu7LPQPy8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_wil = 1 # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = \"/content/epoch_model.checkpoint\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = \"/content/best_model.checkpoint\"#TODO set best model path "
      ],
      "metadata": {
        "id": "FFUj2WUVQP3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb"
      ],
      "metadata": {
        "id": "RCgcYFPWSZOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"a5b7420abbe354e6d0b2f5554b97ee11f327fc92\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ],
      "metadata": {
        "id": "NaMfgXC8RNVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"first-attempt\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = 'oigknwdr', #Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"whisper-finetune\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ],
      "metadata": {
        "id": "FjMI5MgYRN4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Bq87gxHcScqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, optimizer)\n",
        "    valid_loss, val_wil  = validate_model(model, val_loader)\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(val_wil, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'val_wil': val_wil, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    \n",
        "    save_model(model, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
        "    wandb.save('epoch_model.checkpoint')\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if val_wil <= best_wil:\n",
        "        best_wil = val_wil\n",
        "        save_model(model, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
        "        wandb.save('best_model.checkpoint')\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "A3JIKc4mQdP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing with the model"
      ],
      "metadata": {
        "id": "JNukCVCkSl1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
        "# t = tokenizer.encode(\"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\")\n",
        "# print(len(t),t)\n",
        "\n",
        "# import torchaudio\n",
        "# def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
        "#     waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
        "#     if sample_rate != sr:\n",
        "#         waveform = tat.Resample(sr, sample_rate)(waveform)\n",
        "#     return waveform\n",
        "# audio = load_wave('/content/data/recordings/recordings/english1.mp3', sample_rate=16000)\n",
        "# audio = whisper.pad_or_trim(audio.flatten())\n",
        "# mel = whisper.log_mel_spectrogram(audio)\n",
        "# mel = whisper.audio.log_mel_spectrogram('/content/data/recordings/recordings/english10.mp3',padding=N_SAMPLES)\n",
        "# mel = whisper.audio.pad_or_trim(mel, N_FRAMES).to('cpu').to(torch.float32)\n",
        "# options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
        "# m = model.decode(mel,options)\n",
        "# print(len(m.tokens),m.tokens)\n",
        "# d = predict(model, mel)\n",
        "# print(d)"
      ],
      "metadata": {
        "id": "_tF709mAAAqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19937ab6-21a9-40d9-8d07-c12eb5c23977"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79 [16216, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13]\n",
            "[50364, 2555, 818, 45073, 11, 1029, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 50589, 50589, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 12, 65, 404, 13, 50914, 50914, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51114, 51114, 1240, 486, 19555, 613, 721, 493, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3097, 5214, 13, 51414] 89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([50364, 2555, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 11678, 50608, 50608, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 50880, 50880, 3708, 6085, 13, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51120, 51120, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 293, 321, 486, 352, 1677, 720, 10579, 51390, 51390, 412, 264, 3847, 5214, 13, 51440])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "m-j_XwQOG30v",
        "outputId": "f93f1b28-a0aa-4ef6-9dcb-0f7080b96e1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Please call Stella. Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at the train station.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([16216, 818, 45073, 13, 220, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 220, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 220, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "x4kZ_28EHSGx",
        "outputId": "8f6a678e-2ee0-4a6e-9a2a-2ce6274c6c2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "97UX2joWHrbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, tokenizer, mel):\n",
        "  decode_options = dict()\n",
        "  decode_options[\"fp16\"] = (DEVICE == 'cuda')\n",
        "  if decode_options[\"fp16\"]:\n",
        "    dtype = torch.float16\n",
        "  else:\n",
        "    dtype = torch.float32\n",
        "  logprob_threshold = -1.0\n",
        "  no_speech_threshold = 0.6\n",
        "  decode_options[\"language\"] = \"en\"\n",
        "\n",
        "\n",
        "  content_frames = mel.shape[-1] - N_FRAMES\n",
        "\n",
        "  def decode_with_fallback(segment: torch.Tensor) -> whisper.decoding.DecodingResult:\n",
        "      kwargs = {**decode_options}\n",
        "      options = whisper.decoding.DecodingOptions(**kwargs, temperature=0.0)\n",
        "      decode_result = model.decode(segment, options)\n",
        "      return decode_result\n",
        "\n",
        "  seek = 0\n",
        "  input_stride = whisper.utils.exact_div(\n",
        "      N_FRAMES, model.dims.n_audio_ctx\n",
        "  )  # mel frames per output token: 2\n",
        "  time_precision = (\n",
        "      input_stride * HOP_LENGTH / SAMPLE_RATE\n",
        "  )  # time per output token: 0.02 (seconds)\n",
        "  all_tokens = []\n",
        "  all_segments = []\n",
        "  prompt_reset_since = 0\n",
        "\n",
        "  initial_prompt_tokens = []\n",
        "\n",
        "  def new_segment(\n",
        "      *, start: float, end: float, tokens: torch.Tensor, result: whisper.decoding.DecodingResult\n",
        "  ):\n",
        "      tokens = tokens.tolist()\n",
        "      text_tokens = [token for token in tokens if token < tokenizer.eot]\n",
        "      return {\n",
        "          \"seek\": seek,\n",
        "          \"start\": start,\n",
        "          \"end\": end,\n",
        "          \"text\": tokenizer.decode(text_tokens),\n",
        "          \"tokens\": tokens,\n",
        "          \"temperature\": result.temperature,\n",
        "          \"avg_logprob\": result.avg_logprob,\n",
        "          \"compression_ratio\": result.compression_ratio,\n",
        "          \"no_speech_prob\": result.no_speech_prob,\n",
        "      }\n",
        "\n",
        "# show the progress bar when verbose is False (if True, transcribed text will be printed)\n",
        "\n",
        "  while seek < content_frames:\n",
        "      time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
        "      mel_segment = mel[:, seek : seek + N_FRAMES]\n",
        "      segment_size = min(N_FRAMES, content_frames - seek)\n",
        "      segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
        "      mel_segment = whisper.audio.pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
        "\n",
        "      decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
        "      result: whisper.decoding.DecodingResult = decode_with_fallback(mel_segment)\n",
        "      tokens = torch.tensor(result.tokens)\n",
        "\n",
        "      if no_speech_threshold is not None:\n",
        "          # no voice activity check\n",
        "          should_skip = result.no_speech_prob > no_speech_threshold\n",
        "          if (\n",
        "              logprob_threshold is not None\n",
        "              and result.avg_logprob > logprob_threshold\n",
        "          ):\n",
        "              # don't skip if the logprob is high enough, despite the no_speech_prob\n",
        "              should_skip = False\n",
        "\n",
        "          if should_skip:\n",
        "              seek += segment_size  # fast-forward to the next segment boundary\n",
        "              continue\n",
        "\n",
        "      previous_seek = seek\n",
        "      current_segments = []\n",
        "\n",
        "      timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
        "      single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
        "\n",
        "      consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
        "      consecutive.add_(1)\n",
        "      if len(consecutive) > 0:\n",
        "          # if the output contains two consecutive timestamp tokens\n",
        "          slices = consecutive.tolist()\n",
        "          if single_timestamp_ending:\n",
        "              slices.append(len(tokens))\n",
        "\n",
        "          last_slice = 0\n",
        "          for current_slice in slices:\n",
        "              sliced_tokens = tokens[last_slice:current_slice]\n",
        "              start_timestamp_pos = (\n",
        "                  sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
        "              )\n",
        "              end_timestamp_pos = (\n",
        "                  sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
        "              )\n",
        "              current_segments.append(\n",
        "                  new_segment(\n",
        "                      start=time_offset + start_timestamp_pos * time_precision,\n",
        "                      end=time_offset + end_timestamp_pos * time_precision,\n",
        "                      tokens=sliced_tokens,\n",
        "                      result=result,\n",
        "                  )\n",
        "              )\n",
        "              last_slice = current_slice\n",
        "\n",
        "          if single_timestamp_ending:\n",
        "              # single timestamp at the end means no speech after the last timestamp.\n",
        "              seek += segment_size\n",
        "          else:\n",
        "              # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
        "              last_timestamp_pos = (\n",
        "                  tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
        "              )\n",
        "              seek += last_timestamp_pos * input_stride\n",
        "      else:\n",
        "          duration = segment_duration\n",
        "          timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
        "          if (\n",
        "              len(timestamps) > 0\n",
        "              and timestamps[-1].item() != tokenizer.timestamp_begin\n",
        "          ):\n",
        "              # no consecutive timestamps but it has a timestamp; use the last one.\n",
        "              last_timestamp_pos = (\n",
        "                  timestamps[-1].item() - tokenizer.timestamp_begin\n",
        "              )\n",
        "              duration = last_timestamp_pos * time_precision\n",
        "\n",
        "          current_segments.append(\n",
        "              new_segment(\n",
        "                  start=time_offset,\n",
        "                  end=time_offset + duration,\n",
        "                  tokens=tokens,\n",
        "                  result=result,\n",
        "              )\n",
        "          )\n",
        "          seek += segment_size\n",
        "\n",
        "      if result.temperature > 0.5:\n",
        "          # do not feed the prompt tokens if a high temperature was used\n",
        "          prompt_reset_since = len(all_tokens)\n",
        "\n",
        "      # if a segment is instantaneous or does not contain text, clear it\n",
        "      for i, segment in enumerate(current_segments):\n",
        "          if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
        "              segment[\"text\"] = \"\"\n",
        "              segment[\"tokens\"] = []\n",
        "              segment[\"words\"] = []\n",
        "\n",
        "      all_segments.extend(\n",
        "          [\n",
        "              {\"id\": i, **segment}\n",
        "              for i, segment in enumerate(\n",
        "                  current_segments, start=len(all_segments)\n",
        "              )\n",
        "          ]\n",
        "      )\n",
        "      all_tokens.extend(\n",
        "          [token for segment in current_segments for token in segment[\"tokens\"]]\n",
        "      )\n",
        "\n",
        "\n",
        "  return tokenizer.decode(all_tokens)"
      ],
      "metadata": {
        "id": "AQdYISW1oY9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformation = jiwer.Compose([\n",
        "    jiwer.ToLowerCase(),\n",
        "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
        "    jiwer.RemoveMultipleSpaces(),\n",
        "    jiwer.RemovePunctuation(),\n",
        "    jiwer.Strip(),\n",
        "    jiwer.ReduceToListOfListOfWords()\n",
        "]) \n",
        "\n",
        "def wer(target, output):\n",
        "  return jiwer.wer(\n",
        "    target, \n",
        "    output, \n",
        "    truth_transform=text_transformation, \n",
        "    hypothesis_transform=text_transformation)\n",
        "  \n",
        "def wil(target, output):\n",
        "  return jiwer.wil(\n",
        "    target, \n",
        "    output, \n",
        "    truth_transform=text_transformation, \n",
        "    hypothesis_transform=text_transformation)\n"
      ],
      "metadata": {
        "id": "GCZrqF4YNVYV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, cfx, p, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ],
      "metadata": {
        "id": "HcCtP1m8b00S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "thU_GfEZFoVA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8dBbaHC9gNc"
      },
      "source": [
        "## Fetching the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6gi7U8l-rTZ",
        "outputId": "0221be23-450a-48f0-c87d-bf8d9a522f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=2761abf8cabcf8d60f78714a24ae0b4b7d40604aa99ffa664c0530e61002775e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.13\n",
            "    Uninstalling kaggle-1.5.13:\n",
            "      Successfully uninstalled kaggle-1.5.13\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('{\"username\":\"UserName\",\"key\":\"Key\"}') \n",
        "#     # Put your kaggle username & key here\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"sma2023\",\"key\":\"6c819f763f537a6b8bbb60cb11520dbf\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCq_YfEa-nZ_",
        "outputId": "be1d2a2e-248a-4a0b-8df4-27488f4bb84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading speech-accent-archive.zip to /content\n",
            " 99% 860M/865M [00:08<00:00, 65.5MB/s]\n",
            "100% 865M/865M [00:08<00:00, 106MB/s] \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d rtatman/speech-accent-archive\n",
        "!unzip -qo 'speech-accent-archive.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHp_n9i7CQ2p"
      },
      "source": [
        "## Define the two metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3lSOgmCDi9q",
        "outputId": "c571de6c-c08c-43b3-ccc3-254bd197b47c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
            "Collecting rapidfuzz==2.13.7\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.0 rapidfuzz-2.13.7\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwLmLG-kCJkj"
      },
      "outputs": [],
      "source": [
        "import jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jiwer.wer(\"the cat\", \"cat the\")"
      ],
      "metadata": {
        "id": "IXkRPPRCX419",
        "outputId": "8282a50a-69fa-4873-a647-11160e945573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl1z3HKBI7QM"
      },
      "outputs": [],
      "source": [
        "f = open(\"/content/data/reading-passage.txt\")\n",
        "target = \"\"\n",
        "for line in f:\n",
        "  target += line\n",
        "\n",
        "text_transformation = jiwer.Compose([\n",
        "    jiwer.ToLowerCase(),\n",
        "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
        "    jiwer.RemoveMultipleSpaces(),\n",
        "    jiwer.RemovePunctuation(),\n",
        "    # jiwer.ReduceToSingleSentence(),\n",
        "    jiwer.Strip(),\n",
        "    jiwer.SubstituteRegexes({r\"6\": r\"six\", r\"5\": r\"five\", r\"3\": r\"three\"}),\n",
        "    jiwer.ReduceToListOfListOfWords()\n",
        "]) \n",
        "\n",
        "def wer(output):\n",
        "  return jiwer.wer(\n",
        "    target, \n",
        "    output, \n",
        "    truth_transform=text_transformation, \n",
        "    hypothesis_transform=text_transformation)\n",
        "  \n",
        "def wil(output):\n",
        "  return jiwer.wil(\n",
        "    target, \n",
        "    output, \n",
        "    truth_transform=text_transformation, \n",
        "    hypothesis_transform=text_transformation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDF0PSbZNw0k"
      },
      "source": [
        "## Transcribe and recording the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf-4J91DOMdw"
      },
      "outputs": [],
      "source": [
        "# open the speaker_all.csv\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "015b2369aa434a74af9f62546d841984"
          ]
        },
        "id": "FkitHBn6Oba8",
        "outputId": "80de56e7-a474-4c9d-c4d7-c5b75085653f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "015b2369aa434a74af9f62546d841984",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2172 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "speakers = pd.read_csv('/content/data/speakers_all.csv')\n",
        "speakers['wer'] = 1.0\n",
        "speakers['wil'] = 1.0\n",
        "cnt = 0\n",
        "batch_bar   = tqdm(total=len(speakers), dynamic_ncols=True, leave=False, position=0)\n",
        "for index, row in speakers.iterrows():\n",
        "  if row['file_missing?']==False:\n",
        "    file_name = row['filename']\n",
        "    transcription = whisper.transcribe(model = model, audio = '/content/data/recordings/recordings/'+file_name+'.mp3', fp16=False)['text']\n",
        "    speakers.at[index,'wer'] = wer(transcription)\n",
        "    speakers.at[index,'wil'] = wil(transcription)\n",
        "  batch_bar.update()\n",
        "batch_bar.close()\n",
        "speakers.to_csv('results.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}