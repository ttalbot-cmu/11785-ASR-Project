{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k44fylYFktg"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FeMYucCiSr0b"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\" : 1e-5,\n",
    "    \"epochs\" : 2    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yv1FeoIRbG0"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bxd0gRCbRU5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 18:20:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P8    15W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EL5wEwuZRWsi"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb --quiet\n",
    "# !pip install torchsummaryX -q\n",
    "# !pip install mutagen\n",
    "# !pip install jiwer\n",
    "# !pip install git+https://github.com/openai/whisper.git \n",
    "# # on Ubuntu or Debian\n",
    "# !sudo apt update && sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['CUDA_PATH']= '/usr/local/cuda-11.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'CONDA_SHLVL': '1', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'LD_LIBRARY_PATH': '/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64:/usr/local/cuda-11.2/lib:/usr/local/cuda-11.2/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:', 'CONDA_EXE': '/home/ubuntu/anaconda3/bin/conda', 'SSH_CONNECTION': '128.237.82.20 31572 172.31.27.223 22', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'TF_AUTOTUNE_THRESHOLD': '2', 'LANG': 'C.UTF-8', 'OLDPWD': '/', 'CUDA_PATH': '/usr/local/cuda-11.2', 'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '', 'DLAMI_SAVED_CUDA_PATH': '', 'CONDA_PREFIX': '/home/ubuntu/anaconda3/envs/tensorflow2_p310', '_CE_M': '', 'DLAMI_SAVED_LD_LIBRARY_PATH': '/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:', 'XDG_SESSION_ID': '1', 'MODULES_CMD': '/usr/lib/x86_64-linux-gnu/modulecmd.tcl', 'USER': 'ubuntu', 'ENV': '/usr/share/modules/init/profile.sh', 'DLAMI_SAVED_PATH': '/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/condabin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ubuntu/.dl_binaries/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'HOROVOD_NCCL_HOME': '/usr/local/cuda-11.2', 'HOROVOD_GPU_ALLREDUCE': 'NCCL', 'PWD': '/data/home/ubuntu/11785-ASR-Project/finetune', 'HOME': '/home/ubuntu', 'CONDA_PYTHON_EXE': '/home/ubuntu/anaconda3/bin/python', 'LC_CTYPE': 'C.UTF-8', 'SSH_CLIENT': '128.237.82.20 31572 22', 'CUDA_HOME': '/usr/local/cuda-11.2', 'TF_CPP_MIN_LOG_LEVEL': '2', 'BASH_ENV': '/usr/share/modules/init/bash', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', '_CE_CONDA': '', 'GSETTINGS_SCHEMA_DIR': '/home/ubuntu/anaconda3/envs/tensorflow2_p310/share/glib-2.0/schemas', 'LOADEDMODULES': '', 'CONDA_PROMPT_MODIFIER': '(tensorflow2_p310) ', 'SSH_TTY': '/dev/pts/0', 'MAIL': '/var/mail/ubuntu', 'HOROVOD_CUDA_HOME': '/usr/local/cuda-11.2', 'TERM': 'xterm-color', 'SHELL': '/bin/bash', 'PYTHON_VERSION': '3.10', 'SHLVL': '1', 'MANPATH': '/opt/aws/neuron/share/man:', 'MODULEPATH': '/etc/environment-modules/modules:/usr/share/modules/versions:/usr/share/modules/$MODULE_VERSION/modulefiles:/usr/share/modules/modulefiles', 'LOGNAME': 'ubuntu', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'XDG_RUNTIME_DIR': '/run/user/1000', 'MODULEPATH_modshare': '/usr/share/modules/$MODULE_VERSION/modulefiles:1:/etc/environment-modules/modules:1:/usr/share/modules/modulefiles:1:/usr/share/modules/versions:1', 'PATH': '/usr/local/cuda-11.2/bin:/home/ubuntu/anaconda3/envs/tensorflow2_p310/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/condabin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ubuntu/.dl_binaries/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'MODULESHOME': '/usr/share/modules', 'CONDA_DEFAULT_ENV': 'tensorflow2_p310', 'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig:', 'DLAMI_SAVED_CUDA_HOME': '', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'BASH_FUNC_module%%': '() {  _moduleraw \"$*\" 2>&1\\n}', 'BASH_FUNC_switchml%%': '() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}', 'BASH_FUNC__moduleraw%%': '() {  unset _mlre _mlIFS _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre:-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre:-}${_mlv}=\\'`eval \\'echo ${\\'$_mlrv\\':-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre:-}\" ]; then\\n _mlre=\"eval ${_mlre}\";\\n fi;\\n eval `${_mlre:-}/usr/bin/tclsh /usr/lib/x86_64-linux-gnu/modulecmd.tcl bash $*`;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n if [ -n \"${_mlshdbg:-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS _mlshdbg;\\n return $_mlstatus\\n}', '_': '/home/ubuntu/anaconda3/envs/tensorflow2_p310/bin/jupyter-lab', 'JPY_SESSION_NAME': 'whisper.ipynb', 'JPY_PARENT_PID': '5973', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline'})\n"
     ]
    }
   ],
   "source": [
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mE6sHivTH36m",
    "outputId": "175e610a-da43-46ee-b356-1ad728257611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from mutagen.mp3 import MP3\n",
    "import jiwer\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDwpf5XT_1GP"
   },
   "source": [
    "## Load the pretrained Whisper model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6dIP0MrBFks",
    "outputId": "98526568-a2cd-417a-fe52-1343352d0827"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PXfGq1r4ja4m"
   },
   "outputs": [],
   "source": [
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "N_FRAMES = whisper.utils.exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "FRAMES_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "TOKENS_PER_SECOND = whisper.utils.exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtdVyWyGBqH"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "oAEEkVi2FiY0"
   },
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    \n",
    "    def __init__(self, partition, root, tokenizer): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        df = pd.read_csv('common_voice_'+partition+'.csv')\n",
    "        # self.length = len(df)\n",
    "        \n",
    "        self.mfccs = []\n",
    "        self.transcripts = []\n",
    "        self.probs = []\n",
    "        # self.content_frames = []\n",
    "        for index, entry in df.iterrows():\n",
    "            if index == 100:\n",
    "                print(\"100 done\")\n",
    "                break\n",
    "            filepath = root + entry['path']\n",
    "            audio = MP3(filepath)\n",
    "#             if True:\n",
    "            if audio.info.length < 30:\n",
    "                transcript = tokenizer.encode(entry['sentence'])\n",
    "                prob = entry['probs']\n",
    "                mfcc = whisper.audio.log_mel_spectrogram(filepath,padding=N_SAMPLES)\n",
    "                mfcc = whisper.audio.pad_or_trim(mfcc, N_FRAMES).to(DEVICE).to(dtype)\n",
    "                # content_frame = mfcc.shape[-1] - N_FRAMES\n",
    "                self.mfccs.append(mfcc)\n",
    "                self.transcripts.append(transcript)\n",
    "                self.probs.append(prob)\n",
    "                # self.content_frames.append(content_frame)\n",
    "            else:\n",
    "                print(\"too long\")\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
    "\n",
    "        If you didn't do the loading and processing of the data in __init__,\n",
    "        do that here.\n",
    "\n",
    "        Once done, return a tuple of features and labels.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "#         mfcc = torch.FloatTensor(self.mfccs[ind])\n",
    "        mfcc = self.mfccs[ind]\n",
    "        transcript = torch.LongTensor(self.transcripts[ind])\n",
    "        prob = self.probs[ind]\n",
    "#         prob = torch.FloatTensor(self.probs[ind])\n",
    "        # content_frame = torch.LongTensor(self.content_frames[ind])\n",
    "        # return mfcc, transcript, content_frame, prob\n",
    "        return mfcc, transcript, prob\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish. \n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features, \n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = [] # TODO\n",
    "        # batch of output phonemes\n",
    "        batch_transcript = [] # TODO\n",
    "        # batch_content_frame = []\n",
    "        batch_prob = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "\n",
    "        # for mfcc,transcript,content_frame, prob in batch:\n",
    "        for mfcc,transcript, prob in batch:\n",
    "        \n",
    "          batch_mfcc.append(mfcc)\n",
    "          batch_transcript.append(transcript)\n",
    "          # batch_content_frame.append(content_frame)\n",
    "          batch_prob.append(prob)\n",
    "          lengths_mfcc.append(len(mfcc))\n",
    "          lengths_transcript.append(len(transcript))\n",
    "          \n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first= True) # TODO\n",
    "        # lengths_mfcc = batch_mfcc_pad.shape[1] # TODO \n",
    "\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first= True) # TODO\n",
    "        # lengths_transcript = batch_transcript_pad.shape[1] # TODO\n",
    "        \n",
    "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
    "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
    "        #                  -> Would we apply transformation on the validation set as well?\n",
    "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        # return batch_mfcc_pad, batch_transcript_pad, batch_content_frame,  batch_prob, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(batch_prob), torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8EF7Hc4ikiR2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # Increase if your device can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iLYG4uhNkdFD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AudioDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5984/2672277442.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create objects for the dataset class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'toy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# val_data = AudioDataset('validation',root, tokenizer) # TODO : You can either use the same class with some modifications or make a new one :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AudioDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create objects for the dataset class\n",
    "root = '/data/home/ubuntu/cv-corpus-13.0-2023-03-09/en/clips/'\n",
    "train_data = AudioDataset('toy',root, tokenizer) #TODO\n",
    "# val_data = AudioDataset('validation',root, tokenizer) # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "\n",
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = BATCH_SIZE, \n",
    "#     pin_memory  = True,\n",
    "    shuffle     = True,\n",
    "    collate_fn  = train_data.collate_fn\n",
    ")\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     dataset     = val_data, \n",
    "#     num_workers = 2,\n",
    "#     batch_size  = BATCH_SIZE, \n",
    "#     pin_memory  = True,\n",
    "#     shuffle     = False,\n",
    "#     collate_fn  = val_data.collate_fn\n",
    "# )\n",
    "\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vTeyWdnTkruv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 80, 3000])\n",
      "torch.Size([64, 34])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, p, lx, ly = data\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(lx.shape) \n",
    "    print(ly.shape) \n",
    "    print(p.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5zCZ11eSFws"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "BjQVLuTtIMX2"
   },
   "outputs": [],
   "source": [
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
    "        self.asr = model\n",
    "        \n",
    "    \n",
    "    def forward(self, x, lengths_x):\n",
    "        out = self.asr.decode(x,self.options)\n",
    "\n",
    "        return out, lengths_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvLOwGjISJT1"
   },
   "source": [
    "## Criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "xARobkRVLMNr"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True) # Define CTC loss as the criterion. How would the losses be reduced?\n",
    "\n",
    "optimizer =  torch.optim.SGD(model.parameters(),lr = config[\"lr\"]) # What goes in here?\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3) #TODO\n",
    "\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbkJqcWPSQnS"
   },
   "source": [
    "## Train and validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "I3ADWv8gK1aT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, lx, ly, p = data\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = model(x, lx)\n",
    "            # h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "            loss = loss/p\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    val_wil = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            # h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        val_wil += wil(h,y)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(val_wil / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    avg_wil = val_wil/len(val_loader)\n",
    "    return total_loss, avg_wil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNcyukhDSUe7"
   },
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "-ETu7LPQPy8W"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         metric[0]                  : metric[1], \n",
    "         'epoch'                    : epoch}, \n",
    "         path\n",
    "    )\n",
    "\n",
    "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "FFUj2WUVQP3f"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "end = config[\"epochs\"]\n",
    "best_wil = 1 # if you're restarting from some checkpoint, use what you saw there.\n",
    "epoch_model_path = \"/content/epoch_model.checkpoint\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
    "best_model_path = \"/content/best_model.checkpoint\"#TODO set best model path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCgcYFPWSZOF"
   },
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaMfgXC8RNVl"
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"a5b7420abbe354e6d0b2f5554b97ee11f327fc92\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjMI5MgYRN4d"
   },
   "outputs": [],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"first-attempt\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = 'oigknwdr', #Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"whisper-finetune\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq87gxHcScqH"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrmodel = ASRModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "A3JIKc4mQdP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train:   0%|                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GET was unable to find an engine to execute this computation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2718/1467937452.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masrmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_wil\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masrmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2718/308066200.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# h = torch.permute(h, (1, 0, 2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2718/2422215468.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths_x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mn_audio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0maudio_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoder forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_get_audio_features\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         if audio_features.dtype != (\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mmel\u001b[0m \u001b[0mspectrogram\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/whisper/model.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, x, weight, bias)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     ) -> Tensor:\n\u001b[0;32m---> 48\u001b[0;31m         return super()._conv_forward(\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GET was unable to find an engine to execute this computation"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "\n",
    "for epoch in range(0, config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
    "    \n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    train_loss              = train_model(asrmodel, train_loader, optimizer)\n",
    "    valid_loss, val_wil  = validate_model(asrmodel, val_loader)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "    print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(val_wil, valid_loss))\n",
    "\n",
    "\n",
    "#     wandb.log({\n",
    "#         'train_loss': train_loss,  \n",
    "#         'val_wil': val_wil, \n",
    "#         'valid_loss': valid_loss, \n",
    "#         'lr'        : curr_lr\n",
    "#     })\n",
    "    \n",
    "    save_model(arsmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, epoch_model_path)\n",
    "#     wandb.save('epoch_model.checkpoint')\n",
    "    print(\"Saved epoch model\")\n",
    "\n",
    "    if val_wil <= best_wil:\n",
    "        best_wil = val_wil\n",
    "        save_model(asrmodel, optimizer, scheduler, ['val_wil', val_wil], epoch, best_model_path)\n",
    "#         wandb.save('best_model.checkpoint')\n",
    "        print(\"Saved best model\")\n",
    "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNukCVCkSl1m"
   },
   "source": [
    "# Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tF709mAAAqn",
    "outputId": "19937ab6-21a9-40d9-8d07-c12eb5c23977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 [16216, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13]\n",
      "[50364, 2555, 818, 45073, 11, 1029, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 50589, 50589, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 12, 65, 404, 13, 50914, 50914, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51114, 51114, 1240, 486, 19555, 613, 721, 493, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3097, 5214, 13, 51414] 89\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "# t = tokenizer.encode(\"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\")\n",
    "# print(len(t),t)\n",
    "\n",
    "# import torchaudio\n",
    "# def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "#     waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "#     if sample_rate != sr:\n",
    "#         waveform = tat.Resample(sr, sample_rate)(waveform)\n",
    "#     return waveform\n",
    "# audio = load_wave('/content/data/recordings/recordings/english1.mp3', sample_rate=16000)\n",
    "# audio = whisper.pad_or_trim(audio.flatten())\n",
    "# mel = whisper.log_mel_spectrogram(audio)\n",
    "# mel = whisper.audio.log_mel_spectrogram('/content/data/recordings/recordings/english10.mp3',padding=N_SAMPLES)\n",
    "# mel = whisper.audio.pad_or_trim(mel, N_FRAMES).to('cpu').to(torch.float32)\n",
    "# options = whisper.decoding.DecodingOptions(fp16 = False,temperature=0.0)\n",
    "# m = model.decode(mel,options)\n",
    "# print(len(m.tokens),m.tokens)\n",
    "# d = predict(model, mel)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "m-j_XwQOG30v",
    "outputId": "f93f1b28-a0aa-4ef6-9dcb-0f7080b96e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Please call Stella. Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50364, 2555, 818, 45073, 13, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 13, 11678, 50608, 50608, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 50880, 50880, 3708, 6085, 13, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 51120, 51120, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 293, 321, 486, 352, 1677, 720, 10579, 51390, 51390, 412, 264, 3847, 5214, 13, 51440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "x4kZ_28EHSGx",
    "outputId": "8f6a678e-2ee0-4a6e-9a2a-2ce6274c6c2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16216, 818, 45073, 13, 220, 12320, 720, 281, 1565, 613, 721, 365, 720, 490, 264, 3531, 25, 220, 11678, 36316, 295, 4451, 5756, 24494, 11, 1732, 5060, 1061, 17243, 295, 3344, 5399, 11, 293, 1310, 257, 13288, 337, 720, 3708, 6085, 13, 220, 492, 611, 643, 257, 1359, 5900, 12650, 293, 257, 955, 12058, 17259, 337, 264, 2301, 13, 220, 1240, 393, 19555, 613, 721, 666, 1045, 2182, 10405, 11, 293, 321, 486, 352, 1677, 720, 10579, 412, 264, 3847, 5214, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97UX2joWHrbZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQdYISW1oY9V"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, mel):\n",
    "  decode_options = dict()\n",
    "  decode_options[\"fp16\"] = (DEVICE == 'cuda')\n",
    "  if decode_options[\"fp16\"]:\n",
    "    dtype = torch.float16\n",
    "  else:\n",
    "    dtype = torch.float32\n",
    "  logprob_threshold = -1.0\n",
    "  no_speech_threshold = 0.6\n",
    "  decode_options[\"language\"] = \"en\"\n",
    "\n",
    "\n",
    "  content_frames = mel.shape[-1] - N_FRAMES\n",
    "\n",
    "  def decode_with_fallback(segment: torch.Tensor) -> whisper.decoding.DecodingResult:\n",
    "      kwargs = {**decode_options}\n",
    "      options = whisper.decoding.DecodingOptions(**kwargs, temperature=0.0)\n",
    "      decode_result = model.decode(segment, options)\n",
    "      return decode_result\n",
    "\n",
    "  seek = 0\n",
    "  input_stride = whisper.utils.exact_div(\n",
    "      N_FRAMES, model.dims.n_audio_ctx\n",
    "  )  # mel frames per output token: 2\n",
    "  time_precision = (\n",
    "      input_stride * HOP_LENGTH / SAMPLE_RATE\n",
    "  )  # time per output token: 0.02 (seconds)\n",
    "  all_tokens = []\n",
    "  all_segments = []\n",
    "  prompt_reset_since = 0\n",
    "\n",
    "  initial_prompt_tokens = []\n",
    "\n",
    "  def new_segment(\n",
    "      *, start: float, end: float, tokens: torch.Tensor, result: whisper.decoding.DecodingResult\n",
    "  ):\n",
    "      tokens = tokens.tolist()\n",
    "      text_tokens = [token for token in tokens if token < tokenizer.eot]\n",
    "      return {\n",
    "          \"seek\": seek,\n",
    "          \"start\": start,\n",
    "          \"end\": end,\n",
    "          \"text\": tokenizer.decode(text_tokens),\n",
    "          \"tokens\": tokens,\n",
    "          \"temperature\": result.temperature,\n",
    "          \"avg_logprob\": result.avg_logprob,\n",
    "          \"compression_ratio\": result.compression_ratio,\n",
    "          \"no_speech_prob\": result.no_speech_prob,\n",
    "      }\n",
    "\n",
    "# show the progress bar when verbose is False (if True, transcribed text will be printed)\n",
    "\n",
    "  while seek < content_frames:\n",
    "      time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
    "      mel_segment = mel[:, seek : seek + N_FRAMES]\n",
    "      segment_size = min(N_FRAMES, content_frames - seek)\n",
    "      segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
    "      mel_segment = whisper.audio.pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
    "\n",
    "      decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
    "      result: whisper.decoding.DecodingResult = decode_with_fallback(mel_segment)\n",
    "      tokens = torch.tensor(result.tokens)\n",
    "\n",
    "      if no_speech_threshold is not None:\n",
    "          # no voice activity check\n",
    "          should_skip = result.no_speech_prob > no_speech_threshold\n",
    "          if (\n",
    "              logprob_threshold is not None\n",
    "              and result.avg_logprob > logprob_threshold\n",
    "          ):\n",
    "              # don't skip if the logprob is high enough, despite the no_speech_prob\n",
    "              should_skip = False\n",
    "\n",
    "          if should_skip:\n",
    "              seek += segment_size  # fast-forward to the next segment boundary\n",
    "              continue\n",
    "\n",
    "      previous_seek = seek\n",
    "      current_segments = []\n",
    "\n",
    "      timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
    "      single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
    "\n",
    "      consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
    "      consecutive.add_(1)\n",
    "      if len(consecutive) > 0:\n",
    "          # if the output contains two consecutive timestamp tokens\n",
    "          slices = consecutive.tolist()\n",
    "          if single_timestamp_ending:\n",
    "              slices.append(len(tokens))\n",
    "\n",
    "          last_slice = 0\n",
    "          for current_slice in slices:\n",
    "              sliced_tokens = tokens[last_slice:current_slice]\n",
    "              start_timestamp_pos = (\n",
    "                  sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              end_timestamp_pos = (\n",
    "                  sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              current_segments.append(\n",
    "                  new_segment(\n",
    "                      start=time_offset + start_timestamp_pos * time_precision,\n",
    "                      end=time_offset + end_timestamp_pos * time_precision,\n",
    "                      tokens=sliced_tokens,\n",
    "                      result=result,\n",
    "                  )\n",
    "              )\n",
    "              last_slice = current_slice\n",
    "\n",
    "          if single_timestamp_ending:\n",
    "              # single timestamp at the end means no speech after the last timestamp.\n",
    "              seek += segment_size\n",
    "          else:\n",
    "              # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
    "              last_timestamp_pos = (\n",
    "                  tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              seek += last_timestamp_pos * input_stride\n",
    "      else:\n",
    "          duration = segment_duration\n",
    "          timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
    "          if (\n",
    "              len(timestamps) > 0\n",
    "              and timestamps[-1].item() != tokenizer.timestamp_begin\n",
    "          ):\n",
    "              # no consecutive timestamps but it has a timestamp; use the last one.\n",
    "              last_timestamp_pos = (\n",
    "                  timestamps[-1].item() - tokenizer.timestamp_begin\n",
    "              )\n",
    "              duration = last_timestamp_pos * time_precision\n",
    "\n",
    "          current_segments.append(\n",
    "              new_segment(\n",
    "                  start=time_offset,\n",
    "                  end=time_offset + duration,\n",
    "                  tokens=tokens,\n",
    "                  result=result,\n",
    "              )\n",
    "          )\n",
    "          seek += segment_size\n",
    "\n",
    "      if result.temperature > 0.5:\n",
    "          # do not feed the prompt tokens if a high temperature was used\n",
    "          prompt_reset_since = len(all_tokens)\n",
    "\n",
    "      # if a segment is instantaneous or does not contain text, clear it\n",
    "      for i, segment in enumerate(current_segments):\n",
    "          if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
    "              segment[\"text\"] = \"\"\n",
    "              segment[\"tokens\"] = []\n",
    "              segment[\"words\"] = []\n",
    "\n",
    "      all_segments.extend(\n",
    "          [\n",
    "              {\"id\": i, **segment}\n",
    "              for i, segment in enumerate(\n",
    "                  current_segments, start=len(all_segments)\n",
    "              )\n",
    "          ]\n",
    "      )\n",
    "      all_tokens.extend(\n",
    "          [token for segment in current_segments for token in segment[\"tokens\"]]\n",
    "      )\n",
    "\n",
    "\n",
    "  return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GCZrqF4YNVYV"
   },
   "outputs": [],
   "source": [
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(target, output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(target, output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcCtP1m8b00S"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, cfx, p, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():     \n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16. \n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thU_GfEZFoVA"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dBbaHC9gNc"
   },
   "source": [
    "## Fetching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6gi7U8l-rTZ",
    "outputId": "0221be23-450a-48f0-c87d-bf8d9a522f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kaggle==1.5.8\n",
      "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
      "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/59.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=2761abf8cabcf8d60f78714a24ae0b4b7d40604aa99ffa664c0530e61002775e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.13\n",
      "    Uninstalling kaggle-1.5.13:\n",
      "      Successfully uninstalled kaggle-1.5.13\n",
      "Successfully installed kaggle-1.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"UserName\",\"key\":\"Key\"}') \n",
    "#     # Put your kaggle username & key here\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"sma2023\",\"key\":\"6c819f763f537a6b8bbb60cb11520dbf\"}') \n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCq_YfEa-nZ_",
    "outputId": "be1d2a2e-248a-4a0b-8df4-27488f4bb84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading speech-accent-archive.zip to /content\n",
      " 99% 860M/865M [00:08<00:00, 65.5MB/s]\n",
      "100% 865M/865M [00:08<00:00, 106MB/s] \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rtatman/speech-accent-archive\n",
    "!unzip -qo 'speech-accent-archive.zip' -d '/content/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHp_n9i7CQ2p"
   },
   "source": [
    "## Define the two metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3lSOgmCDi9q",
    "outputId": "c571de6c-c08c-43b3-ccc3-254bd197b47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.0 rapidfuzz-2.13.7\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwLmLG-kCJkj"
   },
   "outputs": [],
   "source": [
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkRPPRCX419",
    "outputId": "8282a50a-69fa-4873-a647-11160e945573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jiwer.wer(\"the cat\", \"cat the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl1z3HKBI7QM"
   },
   "outputs": [],
   "source": [
    "f = open(\"/content/data/reading-passage.txt\")\n",
    "target = \"\"\n",
    "for line in f:\n",
    "  target += line\n",
    "\n",
    "text_transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    # jiwer.ReduceToSingleSentence(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.SubstituteRegexes({r\"6\": r\"six\", r\"5\": r\"five\", r\"3\": r\"three\"}),\n",
    "    jiwer.ReduceToListOfListOfWords()\n",
    "]) \n",
    "\n",
    "def wer(output):\n",
    "  return jiwer.wer(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n",
    "  \n",
    "def wil(output):\n",
    "  return jiwer.wil(\n",
    "    target, \n",
    "    output, \n",
    "    truth_transform=text_transformation, \n",
    "    hypothesis_transform=text_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDF0PSbZNw0k"
   },
   "source": [
    "## Transcribe and recording the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf-4J91DOMdw"
   },
   "outputs": [],
   "source": [
    "# open the speaker_all.csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "015b2369aa434a74af9f62546d841984"
     ]
    },
    "id": "FkitHBn6Oba8",
    "outputId": "80de56e7-a474-4c9d-c4d7-c5b75085653f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015b2369aa434a74af9f62546d841984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speakers = pd.read_csv('/content/data/speakers_all.csv')\n",
    "speakers['wer'] = 1.0\n",
    "speakers['wil'] = 1.0\n",
    "cnt = 0\n",
    "batch_bar   = tqdm(total=len(speakers), dynamic_ncols=True, leave=False, position=0)\n",
    "for index, row in speakers.iterrows():\n",
    "  if row['file_missing?']==False:\n",
    "    file_name = row['filename']\n",
    "    transcription = whisper.transcribe(model = model, audio = '/content/data/recordings/recordings/'+file_name+'.mp3', fp16=False)['text']\n",
    "    speakers.at[index,'wer'] = wer(transcription)\n",
    "    speakers.at[index,'wil'] = wil(transcription)\n",
    "  batch_bar.update()\n",
    "batch_bar.close()\n",
    "speakers.to_csv('results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
